var documenterSearchIndex = {"docs":
[{"location":"man/pipeline/#Pipeline-1","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"tutorial/learning/#Training-and-Validation-1","page":"Training and Validation","title":"Training and Validation","text":"","category":"section"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"using Random\nENV[\"COLUMNS\"]=1000\nRandom.seed!(123)","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"Let us continue our discussion by using another dataset. This time,  let's use CMC dataset that are mostly categorical.  CMC is about asking women of their contraceptive choice. The dataset is composed of the following features:","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"using AutoMLPipeline\nusing CSV\ncmcdata = CSV.read(joinpath(dirname(pathof(AutoMLPipeline)),\"../data/cmc.csv\"));\nX = cmcdata[:,1:end-1]\nY = cmcdata[:,end] .|> string\nshow5(df) = first(df,5)\nnothing #hide","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"show5(cmcdata)","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"Let's examine the number of unique instances for each column:","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"[n=>length(unique(x)) for (n,x) in eachcol(cmcdata,true)]","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"Except for Wife's age and Number of children, the other columns have less than five unique instances. Let's create a pipeline to filter those columns and convert them to hotbits and  concatenate them with the standardized scale of the numeric columns.","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"std = SKPreprocessor(\"StandardScaler\")\nohe = OneHotEncoder()\nkohe = SKPreprocessor(\"OneHotEncoder\")\ncatf = CatFeatureSelector()\nnumf = NumFeatureSelector()\ndisc = CatNumDiscriminator(5) # unique instances <= 5 are categories\npcmc = @pipeline disc |> ((catf |> ohe) + (numf |> std)) \ndfcmc = fit_transform!(pcmc,X)\nnothing #hide","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"show5(dfcmc)","category":"page"},{"location":"tutorial/learning/#Evaluate-Learners-with-Same-Pipeline-1","page":"Training and Validation","title":"Evaluate Learners with Same Pipeline","text":"","category":"section"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"You can get a list of sklearners and skpreprocessors by using the following function calls: ","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"sklearners()\nskpreprocessors()","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"Let us evaluate 4 learners using the same preprocessing pipeline:","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"jrf = RandomForest()\nada = SKLearner(\"AdaBoostClassifier\")\nsgd = SKLearner(\"SGDClassifier\")\ntree = PrunedTree()\nnothing #hide","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"using DataFrames\nlearners = DataFrame() \nfor learner in [jrf,ada,sgd,tree]\n  pcmc = @pipeline disc |> ((catf |> ohe) + (numf |> std)) |> learner\n  println(learner.name)\n  mean,sd,folds = crossvalidate(pcmc,X,Y,\"accuracy_score\",5)\n  global learners = vcat(learners,DataFrame(name=learner.name,mean=mean,sd=sd,kfold=folds))\nend;\nnothing #hide","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"@show learners;","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"For this particular pipeline, Adaboost has the best performance followed by RandomForest.","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"Let's extend the pipeline adding Gradient Boost learner and Robust Scaler.","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"rbs = SKPreprocessor(\"RobustScaler\")\ngb = SKLearner(\"GradientBoostingClassifier\")\nlearners = DataFrame() \nfor learner in [jrf,ada,sgd,tree,gb]\n  pcmc = @pipeline disc |> ((catf |> ohe) + (numf |> rbs) + (numf |> std)) |> learner\n  println(learner.name)\n  mean,sd,folds = crossvalidate(pcmc,X,Y,\"accuracy_score\",5)\n  global learners = vcat(learners,DataFrame(name=learner.name,mean=mean,sd=sd,kfold=folds))\nend;\nnothing #hide","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"@show learners;","category":"page"},{"location":"tutorial/learning/#","page":"Training and Validation","title":"Training and Validation","text":"This time, Gradient boost has the best performance.","category":"page"},{"location":"lib/typesfunctions/#lib_decisiontree-1","page":"Types and Functions","title":"Types and Functions","text":"","category":"section"},{"location":"lib/typesfunctions/#Index-1","page":"Types and Functions","title":"Index","text":"","category":"section"},{"location":"lib/typesfunctions/#","page":"Types and Functions","title":"Types and Functions","text":"Modules = [\n   AutoMLPipeline,\n   AutoMLPipeline.AbsTypes,\n   AutoMLPipeline.Pipelines,\n   AutoMLPipeline.BaseFilters,\n   AutoMLPipeline.FeatureSelectors,\n   AutoMLPipeline.EnsembleMethods,\n   AutoMLPipeline.SKPreprocessors,\n   AutoMLPipeline.SKLearners,\n   AutoMLPipeline.DecisionTreeLearners,\n   AutoMLPipeline.SKCrossValidators,\n   AutoMLPipeline.CrossValidators,\n   AutoMLPipeline.Utils\n]","category":"page"},{"location":"lib/typesfunctions/#Descriptions-1","page":"Types and Functions","title":"Descriptions","text":"","category":"section"},{"location":"lib/typesfunctions/#","page":"Types and Functions","title":"Types and Functions","text":"Modules = [\n   AutoMLPipeline,\n   AutoMLPipeline.AbsTypes,\n   AutoMLPipeline.Pipelines,\n   AutoMLPipeline.BaseFilters,\n   AutoMLPipeline.FeatureSelectors,\n   AutoMLPipeline.EnsembleMethods,\n   AutoMLPipeline.SKPreprocessors,\n   AutoMLPipeline.SKLearners,\n   AutoMLPipeline.DecisionTreeLearners,\n   AutoMLPipeline.SKCrossValidators,\n   AutoMLPipeline.CrossValidators,\n   AutoMLPipeline.Utils\n]","category":"page"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{Machine,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(mc::Machine, input::DataFrame, output::Vector)\n\nGeneric trait to be overloaded by different subtypes of Machine.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit_transform!","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit_transform!","text":"fit_transform!(mc::Machine, input::DataFrame, output::Vector=Vector())\n\nDynamic dispatch that calls in sequence fit! and transform! functions.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{Machine,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(mc::Machine, input::DataFrame)\n\nGeneric trait to be overloaded by different subtypes of Machine.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Pipelines.ComboPipeline","page":"Types and Functions","title":"AutoMLPipeline.Pipelines.ComboPipeline","text":"ComboPipeline(machs::Vector{T}) where {T<:Machine}\n\nFeature union pipeline which iteratively calls  fit_transform of each element and concatenate their output into one dataframe.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.Pipelines.Pipeline","page":"Types and Functions","title":"AutoMLPipeline.Pipelines.Pipeline","text":"Pipeline(machs::Vector{T}) where {T<:Machine}\n\nLinearpipeline which iteratively calls and passes the result of fit_transform to the succeeding elements in the pipeline. . \n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.BaseFilters.OneHotEncoder","page":"Types and Functions","title":"AutoMLPipeline.BaseFilters.OneHotEncoder","text":"OneHotEncoder(Dict(\n   # Nominal columns\n   :nominal_columns => Int[],\n\n   # Nominal column values map. Key is column index, value is list of\n   # possible values for that column.\n   :nominal_column_values_map => Dict{Int,Any}()\n))\n\nTransforms instances with nominal features into one-hot form and coerces the instance matrix to be of element type Float64.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.BaseFilters.Imputer","page":"Types and Functions","title":"AutoMLPipeline.BaseFilters.Imputer","text":"Imputer(\n   Dict(\n      # Imputation strategy.\n      # Statistic that takes a vector such as mean or median.\n      :strategy => mean\n   )\n)\n\nImputes NaN values from Float64 features.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.BaseFilters.Wrapper","page":"Types and Functions","title":"AutoMLPipeline.BaseFilters.Wrapper","text":"Wrapper(\n   default_args = Dict(\n      # Transformer to call.\n      :transformer => OneHotEncoder(),\n      # Transformer args.\n      :transformer_args => nothing\n   )\n)\n\nWraps around a AutoMLPipeline transformer.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.BaseFilters.createtransformer","page":"Types and Functions","title":"AutoMLPipeline.BaseFilters.createtransformer","text":"createtransformer(prototype::Transformer, args=Dict())\n\nCreate transformer\n\nprototype: prototype transformer to base new transformer on\noptions: additional options to override prototype's options\n\nReturns: new transformer.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AutoMLPipeline.FeatureSelectors.CatFeatureSelector","page":"Types and Functions","title":"AutoMLPipeline.FeatureSelectors.CatFeatureSelector","text":"CatFeatureSelector(Dict(:name => \"catfeatsel\"))\n\nAutomatically extract categorical columns based on  inferred element types.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.FeatureSelectors.CatNumDiscriminator","page":"Types and Functions","title":"AutoMLPipeline.FeatureSelectors.CatNumDiscriminator","text":"CatNumDiscriminator(\n   Dict(\n      :name => \"catnumdisc\",\n      :maxcategories => 24\n   )\n)\n\nTransform numeric columns to string (as categories)  if the count of their unique elements <= maxcategories.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.FeatureSelectors.FeatureSelector","page":"Types and Functions","title":"AutoMLPipeline.FeatureSelectors.FeatureSelector","text":"FeatureSelector(\n   Dict(\n     :name => \"featureselector\",\n :columns => [col1, col2, ...]\n   )\n)\n\nReturns a dataframe of the selected columns.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.FeatureSelectors.NumFeatureSelector","page":"Types and Functions","title":"AutoMLPipeline.FeatureSelectors.NumFeatureSelector","text":"NumFeatureSelector(Dict(:name=>\"numfeatsel\"))\n\nAutomatically extracts numeric features based on their inferred element types.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.EnsembleMethods.BestLearner","page":"Types and Functions","title":"AutoMLPipeline.EnsembleMethods.BestLearner","text":"BestLearner(\n   Dict(\n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Function to return partitions of instance indices.\n      :partition_generator => (instances, labels) -> kfold(size(instances, 1), 5),\n      # Function that selects the best learner by index.\n      # Arg learner_partition_scores is a (learner, partition) score matrix.\n      :selection_function => (learner_partition_scores) -> findmax(mean(learner_partition_scores, dims=2))[2],      \n      # Score type returned by score() using respective output.\n      :score_type => Real,\n      # Candidate learners.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()],\n      # Options grid for learners, to search through by BestLearner.\n      # Format is [learner_1_options, learner_2_options, ...]\n      # where learner_options is same as a learner's options but\n      # with a list of values instead of scalar.\n      :learner_options_grid => nothing\n   )\n)\n\nSelects best learner from the set by performing a  grid search on learners if grid option is indicated.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.EnsembleMethods.StackEnsemble","page":"Types and Functions","title":"AutoMLPipeline.EnsembleMethods.StackEnsemble","text":"StackEnsemble(\n   Dict(    \n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Set of learners that produce feature space for stacker.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()],\n      # Machine learner that trains on set of learners' outputs.\n      :stacker => RandomForest(),\n      # Proportion of training set left to train stacker itself.\n      :stacker_training_proportion => 0.3,\n      # Provide original features on top of learner outputs to stacker.\n      :keep_original_features => false\n   )\n)\n\nAn ensemble where a 'stack' of learners is used for training and prediction.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.EnsembleMethods.VoteEnsemble","page":"Types and Functions","title":"AutoMLPipeline.EnsembleMethods.VoteEnsemble","text":"VoteEnsemble(\n   Dict( \n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Learners in voting committee.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()]\n   )\n)\n\nSet of machine learners employing majority vote to decide prediction.\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{BestLearner,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(bls::BestLearner, instances::DataFrame, labels::Vector)\n\nTraining phase:\n\nobtain learners as is if grid option is not present \ngenerate learners if grid option is present \nforeach prototype learner, generate learners with specific options found in grid\ngenerate partitions\ntrain each learner on each partition and obtain validation output\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{StackEnsemble,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(se::StackEnsemble, instances::DataFrame, labels::Vector)\n\nTraining phase of the stack of learners.\n\nperform holdout to obtain indices for \npartition learner and stacker training sets\npartition training set for learners and stacker\ntrain all learners\ntrain stacker on learners' outputs\nbuild final model from the trained learners\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{VoteEnsemble,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(ve::VoteEnsemble, instances::DataFrame, labels::Vector)\n\nTraining phase of the ensemble.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{BestLearner,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(bls::BestLearner, instances::DataFrame)\n\nChoose the best learner based on cross-validation results and use it for prediction.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{StackEnsemble,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(se::StackEnsemble, instances::DataFrame)\n\nBuild stacker instances and predict\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{VoteEnsemble,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(ve::VoteEnsemble, instances::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nPrediction phase of the ensemble.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.SKPreprocessors.SKPreprocessor","page":"Types and Functions","title":"AutoMLPipeline.SKPreprocessors.SKPreprocessor","text":"SKPreprocessor(preprocessor::String,args::Dict=Dict())\n\nA wrapper for Scikitlearn preprocessor functions.  Invoking skpreprocessors() will list the acceptable  and supported functions. Please check Scikitlearn documentation for arguments to pass.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.SKLearners.SKLearner","page":"Types and Functions","title":"AutoMLPipeline.SKLearners.SKLearner","text":"SKLearner(learner::String, args::Dict=Dict())\n\nA Scikitlearn wrapper to load the different machine learning models. Invoking sklearners() will list the available learners. Please consult Scikitlearn documentation for arguments to pass.\n\nImplements fit! and transform!. \n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.SKLearners.sklearners-Tuple{}","page":"Types and Functions","title":"AutoMLPipeline.SKLearners.sklearners","text":"sklearners()\n\nList the available scikitlearn machine learners.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.DecisionTreeLearners.Adaboost","page":"Types and Functions","title":"AutoMLPipeline.DecisionTreeLearners.Adaboost","text":"Adaboost(\n  Dict(\n    :output => :class,\n    :num_iterations => 7\n  )\n)\n\nAdaboosted decision tree stumps. See DecisionTree.jl's documentation\n\nHyperparameters:\n\n:num_iterations => 7 (number of iterations of AdaBoost)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.DecisionTreeLearners.PrunedTree","page":"Types and Functions","title":"AutoMLPipeline.DecisionTreeLearners.PrunedTree","text":"PrunedTree(\n  Dict(\n    :purity_threshold => 1.0,\n    :max_depth => -1,\n    :min_samples_leaf => 1,\n    :min_samples_split => 2,\n    :min_purity_increase => 0.0\n  )\n)\n\nDecision tree classifier.   See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:purity_threshold => 1.0 (merge leaves having >=thresh combined purity)\n:max_depth => -1 (maximum depth of the decision tree)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.DecisionTreeLearners.RandomForest","page":"Types and Functions","title":"AutoMLPipeline.DecisionTreeLearners.RandomForest","text":"RandomForest(\n  Dict(\n    :output => :class,\n    :num_subfeatures => 0,\n    :num_trees => 10,\n    :partial_sampling => 0.7,\n    :max_depth => -1\n  )\n)\n\nRandom forest classification.  See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:num_subfeatures => 0  (number of features to consider at random per split)\n:num_trees => 10 (number of trees to train)\n:partial_sampling => 0.7 (fraction of samples to train each tree on)\n:max_depth => -1 (maximum depth of the decision trees)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{Adaboost,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(adaboost::Adaboost, features::DataFrame, labels::Vector)\n\nOptimize the hyperparameters of Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{PrunedTree,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(tree::PrunedTree, features::DataFrame, labels::Vector)\n\nOptimize the hyperparameters of PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{RandomForest,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(forest::RandomForest, features::T, labels::Vector) where {T<:Union{Vector,Matrix,DataFrame}}\n\nOptimize the parameters of the RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{Adaboost,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(adaboost::Adaboost, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nPredict using the optimized hyperparameters of the trained Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{PrunedTree,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(ptree::PrunedTree, features::DataFrame)\n\nPredict using the optimized hyperparameters of the trained PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{RandomForest,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(forest::RandomForest, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nPredict using the optimized hyperparameters of the trained RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.CrossValidators.crossvalidate","page":"Types and Functions","title":"AutoMLPipeline.CrossValidators.crossvalidate","text":"crossvalidate(pl::Machine,X::DataFrame,Y::Vector,sfunc::String=\"balanced_accuracy_score\",nfolds=10)\n\nRuns K-fold cross-validation using balanced accuracy as the default. It support the  following metric:\n\naccuracy_score\nbalancedaccuracyscore\ncohenkappascore\njaccard_score\nmatthews_corrcoef\nhamming_loss\nzerooneloss\nf1_score\nprecision_score\nrecall_score\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AutoMLPipeline.CrossValidators.crossvalidate","page":"Types and Functions","title":"AutoMLPipeline.CrossValidators.crossvalidate","text":"crossvalidate(pl::Machine,X::DataFrame,Y::Vector,pfunc::Function,kfolds=10)\n\nRun K-fold crossvalidation where:\n\npfunc is a performance metric\nX and Y are input and target \n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.aggregatorclskipmissing-Tuple{Function}","page":"Types and Functions","title":"AutoMLPipeline.Utils.aggregatorclskipmissing","text":"aggregatorclskipmissing(fn::Function)\n\nFunction to create aggregator closure with skipmissing features\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.createmachine","page":"Types and Functions","title":"AutoMLPipeline.Utils.createmachine","text":"createmachine(prototype::Machine, options=nothing)\n\nCreate machine\n\nprototype: prototype machine to base new machine on\noptions: additional options to override prototype's options\n\nReturns: new machine\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.find_catnum_columns","page":"Types and Functions","title":"AutoMLPipeline.Utils.find_catnum_columns","text":"find_catnum_columns(instances::DataFrame,maxuniqcat::Int=0)\n\nFinds all categorial and numerical columns. Categorical columns are those that do not have Real type nor do all their elements correspond to Real. Also, columns with size of unique instances are less than maxuniqcat are considered categorical.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.holdout-Tuple{Any,Any}","page":"Types and Functions","title":"AutoMLPipeline.Utils.holdout","text":"holdout(n, right_prop)\n\nHoldout method that partitions a collection into two partitions.\n\nn: Size of collection to partition\nright_prop: Percentage of collection placed in right partition\n\nReturns: two partitions of indices, left and right\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.infer_eltype-Tuple{Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.Utils.infer_eltype","text":"infer_eltype(vector::Vector)\n\nReturns element type of vector unless it is Any. If Any, returns the most specific type that can be inferred from the vector elements.\n\nvector: vector to infer element type on\n\nReturns: inferred element type\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.kfold-Tuple{Any,Any}","page":"Types and Functions","title":"AutoMLPipeline.Utils.kfold","text":"kfold(num_instances, num_partitions)\n\nReturns k-fold partitions.\n\nnum_instances: total number of instances\nnum_partitions: number of partitions required\n\nReturns: training set partition.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.mergedict-Tuple{Dict,Dict}","page":"Types and Functions","title":"AutoMLPipeline.Utils.mergedict","text":"mergedict(first::Dict, second::Dict)\n\nSecond nested dictionary is merged into first.\n\nIf a second dictionary's value as well as the first are both dictionaries, then a merge is conducted between the two inner dictionaries. Otherwise the second's value overrides the first.\n\nfirst: first nested dictionary\nsecond: second nested dictionary\n\nReturns: merged nested dictionary\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.nested_dict_merge-Tuple{Dict,Dict}","page":"Types and Functions","title":"AutoMLPipeline.Utils.nested_dict_merge","text":"nested_dict_merge(first::Dict, second::Dict)\n\nSecond nested dictionary is merged into first.\n\nIf a second dictionary's value as well as the first are both dictionaries, then a merge is conducted between the two inner dictionaries. Otherwise the second's value overrides the first.\n\nfirst: first nested dictionary\nsecond: second nested dictionary\n\nReturns: merged nested dictionary\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.nested_dict_set!-Union{Tuple{T}, Tuple{Dict,Array{T,1},Any}} where T","page":"Types and Functions","title":"AutoMLPipeline.Utils.nested_dict_set!","text":"nested_dict_set!(dict::Dict, keys::Array{T, 1}, value) where {T}\n\nSet value in a nested dictionary.\n\ndict: nested dictionary to assign value\nkeys: keys to access nested dictionaries in sequence\nvalue: value to assign\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.nested_dict_to_tuples-Tuple{Dict}","page":"Types and Functions","title":"AutoMLPipeline.Utils.nested_dict_to_tuples","text":"nested_dict_to_tuples(dict::Dict)\n\nConverts nested dictionary to list of tuples\n\ndict: dictionary that can have other dictionaries as values\n\nReturns: list where elements are ([outer-key, inner-key, ...], value)\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.score-Tuple{Symbol,Array{T,1} where T,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.Utils.score","text":"score(metric::Symbol, actual::Vector, predicted::Vector)\n\nScore learner predictions against ground truth values.\n\nAvailable metrics:\n\n:accuracy\nmetric: metric to assess with\nactual: ground truth values\npredicted: predicted values\n\nReturns: score of learner\n\n\n\n\n\n","category":"method"},{"location":"tutorial/extending/#Extending-AutoMLPipeline-1","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"","category":"section"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"ENV[\"COLUMNS\"]=1000","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"Having a meta-ML package sounds ideal  but not practical  in terms of maintainability and flexibility.  The metapackage becomes a central point of failure and bottleneck. It doesn't subscribe to the KISS philosphy of Unix which encourages decentralization of implementation. As long as the input and output behavior of transformers and learners follow a standard format, they should work without   dependency or communication. By using a consistent input/output interfaces, the passing of information among the elements in the pipeline will not bring any surprises to the receivers and transmitters of information down the line.","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"Because AMPL's symbolic pipeline is based on the idea of Linux pipeline and filters, there is a deliberate effort to follow as much as possible the KISS philosophy by just using two interfaces to be overloaded (fit! and transform!):  input features should be a DataFrame type while the target output should be a Vector type. Transformers fit! function expects only one input argument and ignores the target  argument. On the other hand, the fit! function of any learner  requires both input and target arguments to carry out the  supervised learning phase. For the transform! function, both learners and transformers expect one input argument that both use to apply their learned parameters in transforming the input into either prediction, decomposition, normalization, scaling, etc.","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"Let's extend AMLP by adding CSV reading support embedded in the pipeline. Instead of passing the data in the pipeline argument, we create a csv transformer that passes the data to succeeding elements in the pipeline from a csv file.","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"module FileReaders\n\nusing CSV \nusing DataFrames\n\nusing AutoMLPipeline\nusing AutoMLPipeline.AbsTypes # abstract types (Learners and Transformers)\n\nimport AutoMLPipeline.fit!\nimport AutoMLPipeline.transform!\n\nexport fit!, transform!\nexport CSVReader\n\n# define a user-defined structure for type dispatch\nmutable struct CSVReader <: Transformer\n   name::String\n   model::Dict\n   args::Dict\n\n   function CSVReader(args = Dict(:fname=>\"\"))\n      fname = args[:fname]\n      fname != \"\" || error(\"missing filename.\")  \n      isfile(fname) || error(\"file does not exist.\")\n      new(fname,Dict(),args)\n   end\nend\n\nCSVReader(fname::String) = CSVReader(Dict(:fname=>fname))\n\n# Define fit! which does error checking. You can also make \n# it do nothing and let the transform! function does the\n# the checking and loading. The fit! function is only defined\n# here to make sure there is a fit! dispatch for CSVReader\n# type which is needed in the pipeline call iteration.\nfunction fit!(csvreader::CSVReader, df::DataFrame=DataFrame(), target::Vector=Vector())\n   fname = csvreader.name\n   isfile(fname) || error(\"file does not exist.\")\n   csvreader.model = csvreader.args\nend\n\n# define transform which opens the file and returns a dataframe\nfunction transform!(csvreader::CSVReader, df::DataFrame=DataFrame())\n   fname = csvreader.name\n   df = CSV.read(fname) |> DataFrame\n   df != DataFrame() || error(\"empty dataframe.\")\n   return df\nend\nend\nnothing #hide","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"Let's now load the FileReaders module together with the other AutoMLPipeline modules and create a pipeline that includes the csv reader we just created.","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"using DataFrames\n\nusing AutoMLPipeline, AutoMLPipeline.FeatureSelectors, AutoMLPipeline.EnsembleMethods\nusing AutoMLPipeline.CrossValidators, AutoMLPipeline.DecisionTreeLearners, AutoMLPipeline.Pipelines\nusing AutoMLPipeline.BaseFilters, AutoMLPipeline.SKPreprocessors, AutoMLPipeline.Utils\nusing AutoMLPipeline.SKLearners\n\nusing .FileReaders # load from the Main module\n\n#### Column selector\ncatf = CatFeatureSelector() \nnumf = NumFeatureSelector()\npca = SKPreprocessor(\"PCA\")\nohe = OneHotEncoder()\n\nfname = joinpath(dirname(pathof(AutoMLPipeline)),\"../data/profb.csv\")\ncsvrdr = CSVReader(Dict(:fname=>fname))\n\np1 = @pipeline csvrdr |> (catf + numf)\ndf1 = fit_transform!(p1) # empty argument because input coming from csvreader\nnothing #hide","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"first(df1,5)","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"p2 = @pipeline csvrdr |> (numf |> pca) + (catf |> ohe)  \ndf2 = fit_transform!(p2) # empty argument because input coming from csvreader\nnothing #hide","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"first(df2,5)","category":"page"},{"location":"tutorial/extending/#","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"With the CSVReader extension, csv files can now be directly processed or loaded inside the pipeline and can be used with other existing filters and transformers.","category":"page"},{"location":"tutorial/preprocessing/#Preprocessing-1","page":"Preprocessing","title":"Preprocessing","text":"","category":"section"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Let us start by loading the diabetes dataset:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"using Random\nENV[\"COLUMNS\"]=1000\nRandom.seed!(123)","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"using AutoMLPipeline\nusing CSV\ndiabetesdf = CSV.read(joinpath(dirname(pathof(AutoMLPipeline)),\"../data/diabetes.csv\"))\nX = diabetesdf[:,1:end-1]\nY = diabetesdf[:,end] |> Vector\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"We can check the data by showing the first 5 rows:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"show5(df)=first(df,5); # show first 5 rows\nshow5(diabetesdf)","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"This UCI dataset  is a collection of diagnostic tests among the Pima Indians  to investigate whether the patient shows  sign of diabetes or not based on certain features:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Number of times pregnant\nPlasma glucose concentration a 2 hours in an oral glucose tolerance test\nDiastolic blood pressure (mm Hg)\nTriceps skin fold thickness (mm)\n2-Hour serum insulin (mu U/ml)\nBody mass index (weight in kg/(height in m)^2)\nDiabetes pedigree function\nAge (years)\nClass variable (0 or 1) indicating diabetec or not","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"What is interesting with this dataset is that one or more numeric columns can be categorical and should be hot-bit encoded. One way to verify is  to compute the number of unique instances for each column and look for  columns with relatively smaller count:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"[n=>length(unique(x)) for (n,x) in eachcol(diabetesdf,true)] |> collect","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Among the input columns, preg has only 17 unique instances and it can be treated as a categorical variable. However, its description indicates that the feature refers to the number of times the patient is pregnant and can be considered numerical. With this dillema, we need to figure out which representation provides better performance to our classifier. In order to test the two options, we can use the Feature Discriminator module to filter and transform the preg column to either numeric or categorical and choose the pipeline with the optimal performance.","category":"page"},{"location":"tutorial/preprocessing/#CatNumDiscriminator-for-Detecting-Categorical-Numeric-Features-1","page":"Preprocessing","title":"CatNumDiscriminator for Detecting Categorical Numeric Features","text":"","category":"section"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Transform numeric columns with small unique instances to catergories.","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Let us use CatNumDiscriminator which expects one argument to indicate the maximum number of unique instances in order to consider a particular column as categorical. For the sake of this discussion, let us use its  default value which is 24.","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"using AutoMLPipeline, AutoMLPipeline.FeatureSelectors\nusing AutoMLPipeline.EnsembleMethods, AutoMLPipeline.CrossValidators\nusing AutoMLPipeline.DecisionTreeLearners, AutoMLPipeline.Pipelines\nusing AutoMLPipeline.BaseFilters, AutoMLPipeline.SKPreprocessors\nusing AutoMLPipeline.Utils, AutoMLPipeline.SKLearners\n\ndisc = CatNumDiscriminator(24)\n@pipeline disc\ntr_disc = fit_transform!(disc,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"show5(tr_disc)","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"You may notice that the preg column is converted by the CatNumDiscriminator into String type which can be fed to hot-bit encoder to preprocess  categorical data:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"disc = CatNumDiscriminator(24)\ncatf = CatFeatureSelector()\nohe = OneHotEncoder()\npohe = @pipeline disc |> catf |> ohe\ntr_pohe = fit_transform!(pohe,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"show5(tr_pohe)","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"We have now converted all categorical data into hot-bit encoded values.","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"For a typical scenario, one can consider columns with around 3-10  unique numeric instances to be categorical.  Using CatNumDiscriminator, it is trivial to convert columns of features with small unique instances into categorical and hot-bit encode them as shown below. Let us use 5 as the cut-off and any columns with less than 5 unique instances is converted to hot-bits.","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"using DataFrames\ndf = rand(1:3,100,3) |> DataFrame;\nshow5(df)\ndisc = CatNumDiscriminator(5);\npohe = @pipeline disc |> catf |> ohe;\ntr_pohe = fit_transform!(pohe,df);\nshow5(tr_pohe)","category":"page"},{"location":"tutorial/preprocessing/#Concatenating-Hot-Bits-with-PCA-of-Numeric-Columns-1","page":"Preprocessing","title":"Concatenating Hot-Bits with PCA of Numeric Columns","text":"","category":"section"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Going back to the original diabetes dataset, we can now use the  CatNumDiscriminator to differentiate between categorical  columns and numerical columns and preprocess them based on their  types (String vs Number). Below is the pipeline to convert preg column to hot-bits and use PCA for the numerical features:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"pca = SKPreprocessor(\"PCA\")\ndisc = CatNumDiscriminator(24)\nohe = OneHotEncoder()\ncatf = CatFeatureSelector()\nnumf = NumFeatureSelector()\npl = @pipeline disc |> ((numf |> pca) + (catf |> ohe))\nres_pl = fit_transform!(pl,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"show5(res_pl)","category":"page"},{"location":"tutorial/preprocessing/#Performance-Evaluation-1","page":"Preprocessing","title":"Performance Evaluation","text":"","category":"section"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Let us compare the RF cross-validation result between two options:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"preg column should be categorical vs\npreg column is numerical","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"in predicting diabetes where numerical values are scaled by robust scaler and decomposed by PCA.","category":"page"},{"location":"tutorial/preprocessing/#Option-1:-Assume-All-Numeric-Columns-as-not-Categorical-and-Evaluate-1","page":"Preprocessing","title":"Option 1: Assume All Numeric Columns as not Categorical and Evaluate","text":"","category":"section"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"pca = SKPreprocessor(\"PCA\")\ndt = SKLearner(\"DecisionTreeClassifier\")\nrf = SKLearner(\"RandomForestClassifier\")\nrbs = SKPreprocessor(\"RobustScaler\")\njrf = RandomForest()\nlsvc = SKLearner(\"LinearSVC\")\nohe = OneHotEncoder()\ncatf = CatFeatureSelector()\nnumf = NumFeatureSelector()\ndisc = CatNumDiscriminator(0) # disable turning numeric to categorical features\npl = @pipeline disc |> ((numf |>  pca) + (catf |> ohe)) |> jrf\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"crossvalidate(pl,X,Y,\"accuracy_score\",30)","category":"page"},{"location":"tutorial/preprocessing/#Option-2:-Assume-as-Categorical-Numeric-Columns-24-and-Evaluate-1","page":"Preprocessing","title":"Option 2: Assume as Categorical Numeric Columns <= 24 and Evaluate","text":"","category":"section"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"disc = CatNumDiscriminator(24) # turning numeric to categorical if unique instances <= 24\npl = @pipeline disc |> ((numf |>  pca) + (catf |> ohe)) |> jrf\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"crossvalidate(pl,X,Y,\"accuracy_score\",30)","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"From this evaluation, preg column should be treated as numerical because the corresponding pipeline got better performance. One thing to note is the presence of errors in the crossvalidation performance for the pipeline that treats preg as categorical data. The subset of training data during the kfold validation may contain singularities and evaluation causes some errors due to hotbit encoding that increases data sparsity. The error, however, may be a bug which needs to be addressed in  the future.","category":"page"},{"location":"tutorial/pipeline/#Pipeline-1","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"A tutorial for using the @pipeline expression","category":"page"},{"location":"tutorial/pipeline/#Dataset-1","page":"Pipeline","title":"Dataset","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us start the tutorial by loading the dataset.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using Random\nENV[\"COLUMNS\"]=1000\nRandom.seed!(123)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using AutoMLPipeline\nusing CSV\nprofbdata = CSV.read(joinpath(dirname(pathof(AutoMLPipeline)),\"../data/profb.csv\"))\nX = profbdata[:,2:end] \nY = profbdata[:,1] |> Vector\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"We can check the data by showing the first 5 rows:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(df)=first(df,5); # show first 5 rows\nshow5(profbdata)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"This dataset is a collection of pro football scores with the following variables and their descriptions:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Home/Away = Favored team is at home or away\nFavorite Points = Points scored by the favored team\nUnderdog Points = Points scored by the underdog team\nPointspread = Oddsmaker's points to handicap the favored team\nFavorite Name = Code for favored team's name\nUnderdog name = Code for underdog's name\nYear = 89, 90, or 91","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"note: Note\nFor the purpose of this tutorial, we will use the first column, Home vs Away, as the target variable to be predicted using the other columns as input features. For this target output, we are trying to ask whether the model can learn the patterns from its input features to predict whether the game was played at home or away. Since the input features have both categorical and numerical features, the dataset is a good basis to describe  how to extract these two types of features, preprocessed them, and learn the mapping using a one-liner pipeline expression.","category":"page"},{"location":"tutorial/pipeline/#AutoMLPipeline-Modules-and-Instances-1","page":"Pipeline","title":"AutoMLPipeline Modules and Instances","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Before continuing further with the tutorial, let us load the  necessary modules of AutoMLPipeline:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using AutoMLPipeline, AutoMLPipeline.FeatureSelectors\nusing AutoMLPipeline.EnsembleMethods, AutoMLPipeline.CrossValidators\nusing AutoMLPipeline.DecisionTreeLearners, AutoMLPipeline.Pipelines\nusing AutoMLPipeline.BaseFilters, AutoMLPipeline.SKPreprocessors\nusing AutoMLPipeline.Utils, AutoMLPipeline.SKLearners\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us also create some instances of filters, transformers, and models that we can use to preprocess and model the dataset.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"#### Decomposition\npca = SKPreprocessor(\"PCA\"); fa = SKPreprocessor(\"FactorAnalysis\"); \nica = SKPreprocessor(\"FastICA\")\n\n#### Scaler \nrb = SKPreprocessor(\"RobustScaler\"); pt = SKPreprocessor(\"PowerTransformer\") \nnorm = SKPreprocessor(\"Normalizer\"); mx = SKPreprocessor(\"MinMaxScaler\")\n\n#### categorical preprocessing\nohe = OneHotEncoder()\n\n#### Column selector\ndisc = CatNumDiscriminator()\ncatf = CatFeatureSelector(); numf = NumFeatureSelector()\n\n#### Learners\nrf = SKLearner(\"RandomForestClassifier\"); gb = SKLearner(\"GradientBoostingClassifier\")\nlsvc = SKLearner(\"LinearSVC\"); svc = SKLearner(\"SVC\")\nmlp = SKLearner(\"MLPClassifier\"); ada = SKLearner(\"AdaBoostClassifier\")\njrf = RandomForest(); vote = VoteEnsemble(); stack = StackEnsemble()           \nbest = BestLearner()\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#Processing-Categorical-Features-1","page":"Pipeline","title":"Processing Categorical Features","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"For the first illustration, let us extract categorical features of  the data and output some of them using the pipeline expression  and its interface:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_cat = @pipeline catf \ntr_cat = fit_transform!(pop_cat,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(tr_cat)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"One may notice that instead of using fit! and transform,  the example uses fit_transform! instead. The latter is equivalent to calling fit! and transform in sequence which is handy for examining the final output of the transformation prior to  feeding it to the model.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us now transform the caterical features into one-hotbit-encoding (ohe) and examine the results:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_ohe = @pipeline catf |> ohe\ntr_ohe = fit_transform!(pop_ohe,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(tr_ohe)","category":"page"},{"location":"tutorial/pipeline/#Processing-Numerical-Features-1","page":"Pipeline","title":"Processing Numerical Features","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us have an example of extracting the numerical features of the data using different combinations of filters/transformers:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_rb = @pipeline (numf |> rb)\ntr_rb = fit_transform!(pop_rb,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(tr_rb)","category":"page"},{"location":"tutorial/pipeline/#Concatenating-Extracted-Categorical-and-Numerical-Features-1","page":"Pipeline","title":"Concatenating Extracted Categorical and Numerical Features","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"For typical modeling workflow, input features are combinations of categorical features transformer to one-bit encoding together with numerical features normalized or scaled or transformed by decomposition. ","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Here is an example of a typical input feature:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_com = @pipeline (numf |> norm) + (catf |> ohe)\ntr_com = fit_transform!(pop_com,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(tr_com)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"The column size from 6 grew to 60 after the hot-bit encoding was applied because of the large number of unique instances for the categorical columns. ","category":"page"},{"location":"tutorial/pipeline/#Performance-Evaluation-of-the-Pipeline-1","page":"Pipeline","title":"Performance Evaluation of the Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"We can add a model at the end of the pipeline and evaluate the performance of the entire pipeline by cross-validation.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us use a linear SVC model and evaluate using 5-fold cross-validation.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Random.seed!(12345);\npop_lsvc = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> lsvc;\ntr_lsvc = crossvalidate(pop_lsvc,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"What about using Gradient Boosting model?","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Random.seed!(12345);\npop_gb = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> gb;\ntr_gb = crossvalidate(pop_gb,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"What about using Random Forest model?","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Random.seed!(12345);\npop_rf = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> jrf;\ntr_rf = crossvalidate(pop_rf,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"note: Note\nIt can be inferred from the results that linear SVC has the best performance with respect to the different pipelines evaluated. The compact expression supported by the  pipeline makes testing of the different combination of features  and models trivial. It makes performance evaluation   of the pipeline easily manageable in a systematic way.","category":"page"},{"location":"man/ensemble/#Ensembles-1","page":"Ensembles","title":"Ensembles","text":"","category":"section"},{"location":"man/ensemble/#","page":"Ensembles","title":"Ensembles","text":"dfsfs","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"#AutoMLPipeline-(AMLP)-1","page":"HOME","title":"AutoMLPipeline (AMLP)","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"is a package that makes it trivial to create  complex ML pipeline structures using simple  expressions. AMLP leverages on the built-in macro programming features of Julia to symbolically process, manipulate  pipeline expressions, and automatically discover optimal structures  for machine learning prediction and classification.","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"To illustrate, a typical machine learning workflow that extracts numerical features (numf) for ICA (independent component analysis) and  PCA (principal component analysis) transformations, respectively, concatentated with the hot-bit encoding (ohe) of categorical  features (catf) of a given data for RF modeling can be expressed  in AMLP as:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> model = @pipeline (catf |> ohe) + (numf |> pca) + (numf |> ica) |> rf\njulia> fit!(model,Xtrain,Ytrain)\njulia> prediction = transform!(model,Xtest)\njulia> score(:accuracy,prediction,Ytest)\njulia> crossvalidate(model,X,Y,\"accuracy_score\")\njulia> crossvalidate(model,X,Y,\"balanced_accuracy_score\")","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"You can visualize the pipeline by using AbstractTrees Julia package.","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"# package installation\njulia> using Pkg\njulia> Pkg.add(\"AbstractTrees\")\njulia> Pkg.add(\"AutoMLPipeline\")\n\n# load the packages\njulia> using AbstractTrees\njulia> using AutoMLPipeline\n\njulia> expr = @pipelinex (catf |> ohe) + (numf |> pca) + (numf |> ica) |> rf\n:(Pipeline(ComboPipeline(Pipeline(catf, ohe), Pipeline(numf, pca), Pipeline(numf, ica)), rf))\n\njulia> print_tree(stdout, expr)\n:(Pipeline(ComboPipeline(Pipeline(catf, ohe), Pipeline(numf, pca), Pipeline(numf, ica)), rf))\n :Pipeline\n :(ComboPipeline(Pipeline(catf, ohe), Pipeline(numf, pca), Pipeline(numf, ica)))\n   :ComboPipeline\n   :(Pipeline(catf, ohe))\n     :Pipeline\n     :catf\n     :ohe\n   :(Pipeline(numf, pca))\n     :Pipeline\n     :numf\n     :pca\n   :(Pipeline(numf, ica))\n      :Pipeline\n      :numf\n      :ica\n :rf","category":"page"},{"location":"#Motivations-1","page":"HOME","title":"Motivations","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"The typical workflow in machine learning  classification or prediction requires  some or combination of the following  preprocessing steps together with modeling:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"feature extraction (e.g. ica, pca, svd)\nfeature transformation (e.g. normalization, scaling, ohe)\nfeature selection (anova, correlation)\nmodeling (rf, adaboost, xgboost, lm, svm, mlp)","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Each step has several choices of functions to use together with their corresponding  parameters. Optimizing the performance of the entire pipeline is a combinatorial search of the proper order and combination of preprocessing steps, optimization of their corresponding parameters, together with searching for  the optimal model and its hyper-parameters.","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Because of close dependencies among various steps, we can consider the entire process  to be a pipeline optimization problem (POP). POP requires simultaneous optimization of pipeline structure and parameter adaptation of its elements. As a consequence, having an elegant way to express pipeline structure helps in the analysis and implementation of the optimization routines.","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"The target of future work will be the  implementations of different pipeline  optimization algorithms ranging from  evolutionary approaches, integer programming (discrete choices of POP elements),  tree/graph search, and hyper-parameter search.","category":"page"},{"location":"#Package-Features-1","page":"HOME","title":"Package Features","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pipeline API that allows high-level description of processing workflow\nCommon API wrappers for ML libs including Scikitlearn, DecisionTree, etc\nSymbolic pipeline parsing for easy expression  of complexed pipeline structures\nEasily extensible architecture by overloading just two main interfaces: fit! and transform!\nMeta-ensembles that allows composition of    ensembles of ensembles (recursively if needed)    for robust prediction routines\nCategorical and numerical feature selectors for    specialized preprocessing routines based on types","category":"page"},{"location":"#Installation-1","page":"HOME","title":"Installation","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"AutoMLPipeline is in the Julia Official package registry.  The latest release can be installed at the Julia  prompt using Julia's package management which is triggered by pressing ] at the julia prompt:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> ]\n(v1.0) pkg> add AutoMLPipeline","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> pkg\"add AutoMLPipeline\"","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> Pkg.add(\"AutoMLPipeline\")","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Once AutoMLPipeline is installed, you can  load it by:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using AutoMLPipeline","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> import AutoMLPipeline","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Generally, you will need the different learners/transformers and utils in AMLP for to carry-out the processing and modeling routines. ","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"using AutoMLPipeline \nusing AutoMLPipeline.FeatureSelectors\nusing AutoMLPipeline.EnsembleMethods\nusing AutoMLPipeline.CrossValidators \nusing AutoMLPipeline.DecisionTreeLearners\nusing AutoMLPipeline.Pipelines\nusing AutoMLPipeline.BaseFilters\nusing AutoMLPipeline.SKPreprocessors \nusing AutoMLPipeline.Utils`","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"CSV and DataFrames will be needed in the succeeding examples and should be installed:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"using Pkg\nPkg.add(\"CSV\")\nPkg.add(\"DataFrames\")","category":"page"},{"location":"#Tutorial-Outline-1","page":"HOME","title":"Tutorial Outline","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\n  \"tutorial/pipeline.md\",\n  \"tutorial/preprocessing.md\",\n  \"tutorial/learning.md\",\n  \"tutorial/crossvalidation.md\"\n]\nDepth = 3","category":"page"},{"location":"#Manual-Outline-1","page":"HOME","title":"Manual Outline","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\n  \"man/pipeline.md\",\n  \"man/ensemble.md\",\n  \"man/learners.md\",\n  \"man/preprocessing.md\"\n]\nDepth = 3","category":"page"},{"location":"#ML-Library-1","page":"HOME","title":"ML Library","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\n  \"lib/typesfunctions.md\"\n]","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"","category":"page"},{"location":"man/learners/#Learners-1","page":"Learners","title":"Learners","text":"","category":"section"},{"location":"man/preprocessing/#Ensembles-1","page":"Preprocessing","title":"Ensembles","text":"","category":"section"}]
}
