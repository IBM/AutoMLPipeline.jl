var documenterSearchIndex = {"docs":
[{"location":"man/pipeline/#Pipeline","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"There are three types of Pipelines: Pipeline, ComboPipeline, and Selector Pipeline. The Pipeline (linear pipeline)  performs  sequential evaluation of fit_transform! operation to each of its elements passing the output of previous element as  input to the next element iteratively. ComboPipeline (feature union pipeline) performs dataframe concatenation of the final  outputs of its elements. Selector Pipeline acts as a selector function which outputs the results of the best learner using its internal cross-validation process.","category":"page"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"The Pipeline uses |> symbolic expression while ComboPipeline uses +.  The expression, a |> b, is equivalent to Pipeline(a,b) function call while the expression, a + b, is equivalent to ComboPipeline(a,b). The elements a and b can be transformers, filters, learners or  pipeline themselves.","category":"page"},{"location":"man/pipeline/#Pipeline-Structure","page":"Pipeline","title":"Pipeline Structure","text":"","category":"section"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"The linear pipeline accepts the following variables wrapped in a  Dictionary type argument:","category":"page"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":":name -> alias name for the pipeline\n:machines -> a Vector learners/transformers/pipelines\n:machine_args -> arguments to elements of the pipeline","category":"page"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"For ease of usage, the following function calls are supported:","category":"page"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"Pipeline(args::Dict) -> default init function\nPipeline(Vector{<:Machine},args::Dict=Dict()) -> for passing vectors of learners/transformers\nPipeline(machs::Varargs{Machine}) -> for passing learners/transformers as arguments","category":"page"},{"location":"man/pipeline/#ComboPipeline-Structure","page":"Pipeline","title":"ComboPipeline Structure","text":"","category":"section"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"ComboPipeline or feature union pipeline accepts similar init variables with the the linear pipeline and follows similar helper functions:","category":"page"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"ComboPipeline(args::Dict) -> default init function\nComboPipeline(Vector{<:Machine},args::Dict=Dict()) -> for passing vectors of learners/transformers\nComboPipeline(machs::Varargs{Machine}) -> for passing learners/transformers as arguments","category":"page"},{"location":"man/pipeline/#Selector-Pipeline-Structure","page":"Pipeline","title":"Selector Pipeline Structure","text":"","category":"section"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"Selector Pipeline is based on the BestLearner ensemble. Detailed explanation can be found in:  BestLearner","category":"page"},{"location":"man/pipeline/#Macro-Functions","page":"Pipeline","title":"Macro Functions","text":"","category":"section"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"There are two macro functions available: @pipeline and @pipelinex. The @pipeline macro is used to process pipeline expression and returns the evaluation of the transformed expression.  During its recursive parsing, any occurence of (|>) is converted to CombinePipeline call and + to Pipeline calls. To aid in the understanding of the worfklow, @pipelinex shows the transformed expression during the @pipeline parsing but just before the expression is evaluated. The macro @pipelinex has similar output to that of @macroexpand, i.e., pipelinex expr is equivalent to @macroexpand @pipeline expr.","category":"page"},{"location":"man/pipeline/","page":"Pipeline","title":"Pipeline","text":"Note: Please refer to the Pipeline Tutorial for their usage.","category":"page"},{"location":"man/preprocessors/#Preprocessors","page":"Preprocessors","title":"Preprocessors","text":"","category":"section"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"The design of AMLP is to allow easy extensibility of its processing elements. The choice of Scikitlearn preprocessors in this initial release  is more for demonstration purposes to get a good  narrative of how the various parts of AMLP fits together to solve a particular problem. AMLP has been tested to run with a mixture of transformers and filters from Julia, Scikitlearn, and R's caret in the same pipeline without issues as long as the interfaces are properly implemented for each wrapped functions. As there are loads of preprocessing techniques available, the user is encouraged to create their own wrappers of their favorite implementations  to allow them interoperability with the existing AMLP implementations.","category":"page"},{"location":"man/preprocessors/#SKPreprocessor-Structure","page":"Preprocessors","title":"SKPreprocessor Structure","text":"","category":"section"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"    SKPreprocessor(args=Dict(\n       :name => \"skprep\",\n       :preprocessor => \"PCA\",\n       :impl_args => Dict()\n      )\n    )\n\nHelper Function:\n   SKPreprocessor(preprocessor::String,args::Dict=Dict())","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"SKPreprocessor maintains a dictionary of pre-processors and dynamically load them based on the :preprocessor name passed during its initialization. The  :impl_args is a dictionary of parameters to be passed as arguments to the Scikitlearn preprocessor. ","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"note: Note\nPlease consult the documentation in Scikitlearn  for what arguments to pass relative to the chosen preprocessor.","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"Let's try PCA with 2 components decomposition and random state initialized at 0.","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"using AutoMLPipeline\n\niris = getiris()\nX=iris[:,1:4]\n\npca = SKPreprocessor(\"PCA\",Dict(:n_components=>2,:random_state=>0))\nrespca = fit_transform!(pca,X)\nnothing #hide","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"first(respca,5)","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"Let's try ICA with 3 components decomposition and whitening:","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"ica = SKPreprocessor(\"FastICA\",Dict(:n_components=>3,:whiten=>true))\nresica = fit_transform!(ica,X)\nnothing #hide","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"first(resica,5)","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"To get a listing of available preprocessors, use the skpreprocessors() function:","category":"page"},{"location":"man/preprocessors/","page":"Preprocessors","title":"Preprocessors","text":"skpreprocessors()","category":"page"},{"location":"tutorial/learning/#Training-and-Validation","page":"Training and Validation","title":"Training and Validation","text":"","category":"section"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"Let us continue our discussion by using another dataset. This time,  let's use CMC dataset that are mostly categorical.  CMC is about asking women of their contraceptive choice. The dataset is composed of the following features:","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"using AutoMLPipeline\nusing CSV\nusing DataFrames\n\ncmcdata = CSV.File(joinpath(dirname(pathof(AutoMLPipeline)),\"../data/cmc.csv\")) |> DataFrame;\nX = cmcdata[:,1:end-1]\nY = cmcdata[:,end] .|> string\nshow5(df) = first(df,5)\nnothing #hide","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"show5(cmcdata)","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"Let's examine the number of unique instances for each column:","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"DataFrame(hcat([length(unique(n)) for n in eachcol(cmcdata)],names(cmcdata)),:auto)","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"Except for Wife's age and Number of children, the other columns have less than five unique instances. Let's create a pipeline to filter those columns and convert them to hot-bits and  concatenate them with the standardized scale of the numeric columns.","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"std = SKPreprocessor(\"StandardScaler\")\nohe = OneHotEncoder()\nkohe = SKPreprocessor(\"OneHotEncoder\")\ncatf = CatFeatureSelector()\nnumf = NumFeatureSelector()\ndisc = CatNumDiscriminator(5) # unique instances <= 5 are categories\npcmc = @pipeline disc |> ((catf |> ohe) + (numf |> std)) \ndfcmc = fit_transform!(pcmc,X)\nnothing #hide","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"show5(dfcmc)","category":"page"},{"location":"tutorial/learning/#Evaluate-Learners-with-Same-Pipeline","page":"Training and Validation","title":"Evaluate Learners with Same Pipeline","text":"","category":"section"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"You can get a list of sklearners and skpreprocessors by using the following function calls: ","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"sklearners()\nskpreprocessors()","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"Let us evaluate 4 learners using the same preprocessing pipeline:","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"jrf = RandomForest()\nada = SKLearner(\"AdaBoostClassifier\")\nsgd = SKLearner(\"SGDClassifier\")\ntree = PrunedTree()\nnothing #hide","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"using DataFrames: DataFrame, nrow,ncol\n\nlearners = DataFrame() \nfor learner in [jrf,ada,sgd,tree]\n  pcmc = @pipeline disc |> ((catf |> ohe) + (numf |> std)) |> learner\n  println(learner.name)\n  mean,sd,folds = crossvalidate(pcmc,X,Y,\"accuracy_score\",5)\n  global learners = vcat(learners,DataFrame(name=learner.name,mean=mean,sd=sd,kfold=folds))\nend;\nnothing #hide","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"@show learners;","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"For this particular pipeline, Adaboost has the best performance followed by RandomForest.","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"Let's extend the pipeline adding Gradient Boost learner and Robust Scaler.","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"rbs = SKPreprocessor(\"RobustScaler\")\ngb = SKLearner(\"GradientBoostingClassifier\")\nlearners = DataFrame() \nfor learner in [jrf,ada,sgd,tree,gb]\n  pcmc = @pipeline disc |> ((catf |> ohe) + (numf |> rbs) + (numf |> std)) |> learner\n  println(learner.name)\n  mean,sd,folds = crossvalidate(pcmc,X,Y,\"accuracy_score\",5)\n  global learners = vcat(learners,DataFrame(name=learner.name,mean=mean,sd=sd,kfold=folds))\nend;\nnothing #hide","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"@show learners;","category":"page"},{"location":"tutorial/learning/","page":"Training and Validation","title":"Training and Validation","text":"This time, Gradient boost has the best performance.","category":"page"},{"location":"lib/typesfunctions/#Index","page":"Types and Functions","title":"Index","text":"","category":"section"},{"location":"lib/typesfunctions/","page":"Types and Functions","title":"Types and Functions","text":"Order   = [:module,:type,:function]\nModules = [\n   AutoMLPipeline.AbsTypes,\n\tAutoMLPipeline.BaselineModels,\n   AutoMLPipeline.BaseFilters,\n   AutoMLPipeline.Pipelines,\n\tAutoMLPipeline.NARemovers,\n   AutoMLPipeline.CrossValidators,\n   AutoMLPipeline.DecisionTreeLearners,\n   AutoMLPipeline.EnsembleMethods,\n   AutoMLPipeline.FeatureSelectors,\n   AutoMLPipeline.SKLearners,\n   AutoMLPipeline.SKPreprocessors,\n   AutoMLPipeline.SKCrossValidators,\n   AutoMLPipeline.Utils\n]","category":"page"},{"location":"lib/typesfunctions/#Descriptions","page":"Types and Functions","title":"Descriptions","text":"","category":"section"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-Tuple{Machine, DataFrames.DataFrame, Vector}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(mc::Machine, input::DataFrame, output::Vector)\n\nGeneric trait to be overloaded by different subtypes of Machine. Multiple dispatch for fit!.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit_transform!","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit_transform!","text":"fit_transform!(mc::Machine, input::DataFrame, output::Vector)\n\nDynamic dispatch that calls in sequence fit! and transform! functions.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Machine, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(mc::Machine, input::DataFrame)\n\nGeneric trait to be overloaded by different subtypes of Machine. Multiple dispatch for transform!.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.BaselineModels.Baseline","page":"Types and Functions","title":"AMLPipelineBase.BaselineModels.Baseline","text":"Baseline(\n   default_args = Dict(\n       :name => \"baseline\",\n      :output => :class,\n      :strat => mode\n   )\n)\n\nBaseline model that returns the mode during classification.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.BaselineModels.Baseline-Tuple{String}","page":"Types and Functions","title":"AMLPipelineBase.BaselineModels.Baseline","text":"Baseline(name::String,opt...)\n\nHelper function\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.BaselineModels.Identity","page":"Types and Functions","title":"AMLPipelineBase.BaselineModels.Identity","text":"Identity(args=Dict())\n\nReturns the input as output.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.BaselineModels.Identity-Tuple{String}","page":"Types and Functions","title":"AMLPipelineBase.BaselineModels.Identity","text":"Identity(name::String,opt...)\n\nHelper function\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(idy::Identity,x::DataFrame,y::Vector)\n\nDoes nothing.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-Tuple{Baseline, DataFrames.DataFrame, Vector}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(bsl::Baseline,x::DataFrame,y::Vector)\n\nGet the mode of the training data.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(idy::Identity,x::DataFrame)\n\nReturn the input as output.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Baseline, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(bsl::Baseline,x::DataFrame)\n\nReturn the mode in classification.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.BaseFilters.Imputer","page":"Types and Functions","title":"AMLPipelineBase.BaseFilters.Imputer","text":"Imputer(\n   Dict(\n      # Imputation strategy.\n      # Statistic that takes a vector such as mean or median.\n      :strategy => mean\n   )\n)\n\nImputes NaN values from Float64 features.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.BaseFilters.OneHotEncoder","page":"Types and Functions","title":"AMLPipelineBase.BaseFilters.OneHotEncoder","text":"OneHotEncoder(Dict(\n   # Nominal columns\n   :nominal_columns => Int[],\n\n   # Nominal column values map. Key is column index, value is list of\n   # possible values for that column.\n   :nominal_column_values_map => Dict{Int,Any}()\n))\n\nTransforms myinstances with nominal features into one-hot form and coerces the instance matrix to be of element type Float64.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.BaseFilters.Wrapper","page":"Types and Functions","title":"AMLPipelineBase.BaseFilters.Wrapper","text":"Wrapper(\n   default_args = Dict(\n      :name => \"ohe-wrapper\",\n      # Transformer to call.\n      :transformer => OneHotEncoder(),\n      # Transformer args.\n      :transformer_args => Dict()\n   )\n)\n\nWraps around a transformer.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.BaseFilters.createtransformer","page":"Types and Functions","title":"AMLPipelineBase.BaseFilters.createtransformer","text":"createtransformer(prototype::Transformer, args=Dict())\n\nCreate transformer\n\nprototype: prototype transformer to base new transformer on\noptions: additional options to override prototype's options\n\nReturns: new transformer.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AMLPipelineBase.Pipelines.ComboPipeline","page":"Types and Functions","title":"AMLPipelineBase.Pipelines.ComboPipeline","text":"ComboPipeline(machs::Vector{T}) where {T<:Machine}\n\nFeature union pipeline which iteratively calls  fit_transform of each element and concatenate their output into one dataframe.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.Pipelines.Pipeline","page":"Types and Functions","title":"AMLPipelineBase.Pipelines.Pipeline","text":"Pipeline(machs::Vector{<:Machine},args::Dict=Dict())\n\nLinear pipeline which iteratively calls and passes the result of fit_transform to the succeeding elements in the pipeline.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.Pipelines.Pipeline-Tuple{Vararg{Machine}}","page":"Types and Functions","title":"AMLPipelineBase.Pipelines.Pipeline","text":"Pipeline(machs::Vararg{Machine})\n\nHelper function for Pipeline structure.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Pipelines.Pipeline-Tuple{Vector{<:Machine}, Dict}","page":"Types and Functions","title":"AMLPipelineBase.Pipelines.Pipeline","text":"Pipeline(machs::Vector{<:Machine},args::Dict=Dict())\n\nHelper function for Pipeline structure.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.NARemovers.NARemover","page":"Types and Functions","title":"AMLPipelineBase.NARemovers.NARemover","text":"NARemover(\n  Dict(\n    :name => \"nadetect\",\n    :acceptance => 0.10 # tolerable NAs percentage\n  )\n)\n\nRemoves columns with NAs greater than acceptance rate. This assumes that it processes columns of features.  The output column should not be part of input to avoid it being excluded if it fails the acceptance critera.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.NARemovers.NARemover-Tuple{Float64}","page":"Types and Functions","title":"AMLPipelineBase.NARemovers.NARemover","text":"NARemover(acceptance::Float64)\n\nHelper function for NARemover.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-2","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(nad::NARemover,features::DataFrame,labels::Vector=[])\n\nChecks and exit of df is empty\n\nArguments\n\nnad::NARemover: custom type\nfeatures::DataFrame: input\nlabels::Vector=[]: \n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{NARemover, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(nad::NARemover,nfeatures::DataFrame)\n\nRemoves columns with NAs greater than acceptance rate.\n\nArguments\n\nnad::NARemover: custom type\nnfeatures::DataFrame: input\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.CrossValidators.crossvalidate-Tuple{Machine, DataFrames.DataFrame, Vector, Function, Int64, Bool}","page":"Types and Functions","title":"AMLPipelineBase.CrossValidators.crossvalidate","text":"crossvalidate(pl::Machine,X::DataFrame,Y::Vector,pfunc::Function,kfolds=10)\n\nRun K-fold crossvalidation where:\n\npfunc is a performance metric\nX and Y are input and target \n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.DecisionTreeLearners.Adaboost","page":"Types and Functions","title":"AMLPipelineBase.DecisionTreeLearners.Adaboost","text":"Adaboost(\n  Dict(\n    :output => :class,\n    :num_iterations => 7\n  )\n)\n\nAdaboosted decision tree stumps. See DecisionTree.jl's documentation\n\nHyperparameters:\n\n:num_iterations => 7 (number of iterations of AdaBoost)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.DecisionTreeLearners.PrunedTree","page":"Types and Functions","title":"AMLPipelineBase.DecisionTreeLearners.PrunedTree","text":"PrunedTree(\n  Dict(\n    :purity_threshold => 1.0,\n    :max_depth => -1,\n    :min_samples_leaf => 1,\n    :min_samples_split => 2,\n    :min_purity_increase => 0.0\n  )\n)\n\nDecision tree classifier.   See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:purity_threshold => 1.0 (merge leaves having >=thresh combined purity)\n:max_depth => -1 (maximum depth of the decision tree)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.DecisionTreeLearners.RandomForest","page":"Types and Functions","title":"AMLPipelineBase.DecisionTreeLearners.RandomForest","text":"RandomForest(\n  Dict(\n    :output => :class,\n    :num_subfeatures => 0,\n    :num_trees => 10,\n    :partial_sampling => 0.7,\n    :max_depth => -1\n  )\n)\n\nRandom forest classification.  See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:num_subfeatures => 0  (number of features to consider at random per split)\n:num_trees => 10 (number of trees to train)\n:partial_sampling => 0.7 (fraction of samples to train each tree on)\n:max_depth => -1 (maximum depth of the decision trees)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-Tuple{Adaboost, DataFrames.DataFrame, Vector}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(adaboost::Adaboost, features::DataFrame, labels::Vector)\n\nOptimize the hyperparameters of Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-Tuple{PrunedTree, DataFrames.DataFrame, Vector}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(tree::PrunedTree, features::DataFrame, labels::Vector)\n\nOptimize the hyperparameters of PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-Tuple{RandomForest, DataFrames.DataFrame, Vector}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(forest::RandomForest, features::DataFrame, labels::Vector)\n\nOptimize the parameters of the RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Adaboost, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(adaboost::Adaboost, features::DataFrame)\n\nPredict using the optimized hyperparameters of the trained Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{PrunedTree, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(ptree::PrunedTree, features::DataFrame)\n\nPredict using the optimized hyperparameters of the trained PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{RandomForest, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(forest::RandomForest, features::DataFrame)\n\nPredict using the optimized hyperparameters of the trained RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.EnsembleMethods.BestLearner","page":"Types and Functions","title":"AMLPipelineBase.EnsembleMethods.BestLearner","text":"BestLearner(\n   Dict(\n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Function to return partitions of instance indices.\n      :partition_generator => (instances, labels) -> kfold(size(instances, 1), 5),\n      # Function that selects the best learner by index.\n      # Arg learner_partition_scores is a (learner, partition) score matrix.\n      :selection_function => (learner_partition_scores) -> findmax(mean(learner_partition_scores, dims=2))[2],      \n      # Score type returned by score() using respective output.\n      :score_type => Real,\n      # Candidate learners.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()],\n      # Options grid for learners, to search through by BestLearner.\n      # Format is [learner_1_options, learner_2_options, ...]\n      # where learner_options is same as a learner's options but\n      # with a list of values instead of scalar.\n      :learner_options_grid => nothing\n   )\n)\n\nSelects best learner from the set by performing a  grid search on learners if grid option is indicated.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.EnsembleMethods.StackEnsemble","page":"Types and Functions","title":"AMLPipelineBase.EnsembleMethods.StackEnsemble","text":"StackEnsemble(\n   Dict(    \n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Set of learners that produce feature space for stacker.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()],\n      # Machine learner that trains on set of learners' outputs.\n      :stacker => RandomForest(),\n      # Proportion of training set left to train stacker itself.\n      :stacker_training_proportion => 0.3,\n      # Provide original features on top of learner outputs to stacker.\n      :keep_original_features => false\n   )\n)\n\nAn ensemble where a 'stack' of learners is used for training and prediction.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.EnsembleMethods.VoteEnsemble","page":"Types and Functions","title":"AMLPipelineBase.EnsembleMethods.VoteEnsemble","text":"VoteEnsemble(\n   Dict( \n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Learners in voting committee.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()]\n   )\n)\n\nSet of machine learners employing majority vote to decide prediction.\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-Tuple{BestLearner, DataFrames.DataFrame, Vector}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(bls::BestLearner, instances::DataFrame, labels::Vector)\n\nTraining phase:\n\nobtain learners as is if grid option is not present \ngenerate learners if grid option is present \nforeach prototype learner, generate learners with specific options found in grid\ngenerate partitions\ntrain each learner on each partition and obtain validation output\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-Tuple{StackEnsemble, DataFrames.DataFrame, Vector}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(se::StackEnsemble, instances::DataFrame, labels::Vector)\n\nTraining phase of the stack of learners.\n\nperform holdout to obtain indices for \npartition learner and stacker training sets\npartition training set for learners and stacker\ntrain all learners\ntrain stacker on learners' outputs\nbuild final model from the trained learners\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.fit!-Tuple{VoteEnsemble, DataFrames.DataFrame, Vector}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(ve::VoteEnsemble, instances::DataFrame, labels::Vector)\n\nTraining phase of the ensemble.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{BestLearner, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(bls::BestLearner, instances::DataFrame)\n\nChoose the best learner based on cross-validation results and use it for prediction.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{StackEnsemble, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(se::StackEnsemble, instances::DataFrame)\n\nBuild stacker instances and predict\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.AbsTypes.transform!-Tuple{VoteEnsemble, DataFrames.DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(ve::VoteEnsemble, instances::DataFrame)\n\nPrediction phase of the ensemble.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.FeatureSelectors.CatFeatureSelector","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.CatFeatureSelector","text":"CatFeatureSelector(Dict(:name => \"catf\"))\n\nAutomatically extract categorical columns based on  inferred element types.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.FeatureSelectors.CatNumDiscriminator","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.CatNumDiscriminator","text":"CatNumDiscriminator(\n   Dict(\n      :name => \"catnumdisc\",\n      :maxcategories => 24\n   )\n)\n\nTransform numeric columns to string (as categories)  if the count of their unique elements <= maxcategories.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.FeatureSelectors.CatNumDiscriminator-Tuple{Int64}","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.CatNumDiscriminator","text":"CatNumDiscriminator(maxcat::Int)\n\nHelper function for CatNumDiscriminator.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.FeatureSelectors.FeatureSelector","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.FeatureSelector","text":"FeatureSelector(\n   Dict(\n     :name => \"featureselector\",\n     :columns => [col1, col2, ...]\n   )\n)\n\nReturns a dataframe of the selected columns.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.FeatureSelectors.FeatureSelector-Tuple{Vararg{Int64}}","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.FeatureSelector","text":"FeatureSelector(cols::Vararg{Int})\n\nHelper function for FeatureSelector.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.FeatureSelectors.FeatureSelector-Tuple{Vector{Int64}}","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.FeatureSelector","text":"FeatureSelector(cols::Vector{Int})\n\nHelper function for FeatureSelector.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.FeatureSelectors.NumFeatureSelector","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.NumFeatureSelector","text":"NumFeatureSelector(Dict(:name=>\"numfeatsel\"))\n\nAutomatically extracts numeric features based on their inferred element types.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.SKLearners.SKLearner","page":"Types and Functions","title":"AutoMLPipeline.SKLearners.SKLearner","text":"SKLearner(learner::String, args::Dict=Dict())\n\nA Scikitlearn wrapper to load the different machine learning models. Invoking sklearners() will list the available learners. Please consult Scikitlearn documentation for arguments to pass.\n\nImplements fit! and transform!. \n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.SKLearners.sklearners-Tuple{}","page":"Types and Functions","title":"AutoMLPipeline.SKLearners.sklearners","text":"function sklearners()\n\nList the available scikitlearn machine learners.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.SKPreprocessors.SKPreprocessor","page":"Types and Functions","title":"AutoMLPipeline.SKPreprocessors.SKPreprocessor","text":"SKPreprocessor(preprocessor::String,args::Dict=Dict())\n\nA wrapper for Scikitlearn preprocessor functions.  Invoking skpreprocessors() will list the acceptable  and supported functions. Please check Scikitlearn documentation for arguments to pass.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AMLPipelineBase.CrossValidators.crossvalidate-Tuple{Machine, DataFrames.DataFrame, Vector, String}","page":"Types and Functions","title":"AMLPipelineBase.CrossValidators.crossvalidate","text":"crossvalidate(pl::Machine,X::DataFrame,Y::Vector,sfunc::String=\"balanced_accuracy_score\";nfolds=10,verbose=true)\n\nRuns K-fold cross-validation using balanced accuracy as the default. It support the  following metrics for classification:\n\n\"accuracy_score\"\n\"balancedaccuracyscore\"\n\"cohenkappascore\"\n\"jaccard_score\"\n\"matthews_corrcoef\"\n\"hamming_loss\"\n\"zerooneloss\"\n\"f1_score\"\n\"precision_score\"\n\"recall_score\"\n\nand the following metrics for regression:\n\n\"meansquarederror\"\n\"meansquaredlog_error\"\n\"medianabsoluteerror\"\n\"r2_score\"\n\"max_error\"\n\"explainedvariancescore\"\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.aggregatorclskipmissing-Tuple{Function}","page":"Types and Functions","title":"AMLPipelineBase.Utils.aggregatorclskipmissing","text":"aggregatorclskipmissing(fn::Function)\n\nFunction to create aggregator closure with skipmissing features\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.createmachine","page":"Types and Functions","title":"AMLPipelineBase.Utils.createmachine","text":"createmachine(prototype::Machine, options=nothing)\n\nCreate machine\n\nprototype: prototype machine to base new machine on\nargs: additional options to override prototype's options\n\nReturns: new machine\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.find_catnum_columns","page":"Types and Functions","title":"AMLPipelineBase.Utils.find_catnum_columns","text":"find_catnum_columns(instances::DataFrame,maxuniqcat::Int=0)\n\nFinds all categorial and numerical columns. Categorical columns are those that do not have Real type nor do all their elements correspond to Real. Also, columns with size of unique instances are less than maxuniqcat are considered categorical.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.holdout-Tuple{Any, Any}","page":"Types and Functions","title":"AMLPipelineBase.Utils.holdout","text":"holdout(n, right_prop)\n\nHoldout method that partitions a collection into two partitions.\n\nn: Size of collection to partition\nright_prop: Percentage of collection placed in right partition\n\nReturns: two partitions of indices, left and right\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.infer_eltype-Tuple{Vector}","page":"Types and Functions","title":"AMLPipelineBase.Utils.infer_eltype","text":"infer_eltype(vector::Vector)\n\nReturns element type of vector unless it is Any. If Any, returns the most specific type that can be inferred from the vector elements.\n\nvector: vector to infer element type on\n\nReturns: inferred element type\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.kfold-Tuple{Any, Any}","page":"Types and Functions","title":"AMLPipelineBase.Utils.kfold","text":"kfold(num_instances, num_partitions)\n\nReturns k-fold partitions.\n\nnum_instances: total number of instances\nnum_partitions: number of partitions required\n\nReturns: training set partition.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.nested_dict_merge-Tuple{Dict, Dict}","page":"Types and Functions","title":"AMLPipelineBase.Utils.nested_dict_merge","text":"nested_dict_merge(first::Dict, second::Dict)\n\nSecond nested dictionary is merged into first.\n\nIf a second dictionary's value as well as the first are both dictionaries, then a merge is conducted between the two inner dictionaries. Otherwise the second's value overrides the first.\n\nfirst: first nested dictionary\nsecond: second nested dictionary\n\nReturns: merged nested dictionary\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.nested_dict_set!-Union{Tuple{T}, Tuple{Dict, Vector{T}, Any}} where T","page":"Types and Functions","title":"AMLPipelineBase.Utils.nested_dict_set!","text":"nested_dict_set!(dict::Dict, keys::Array{T, 1}, value) where {T}\n\nSet value in a nested dictionary.\n\ndict: nested dictionary to assign value\nkeys: keys to access nested dictionaries in sequence\nvalue: value to assign\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.nested_dict_to_tuples-Tuple{Dict}","page":"Types and Functions","title":"AMLPipelineBase.Utils.nested_dict_to_tuples","text":"nested_dict_to_tuples(dict::Dict)\n\nConverts nested dictionary to list of tuples\n\ndict: dictionary that can have other dictionaries as values\n\nReturns: list where elements are ([outer-key, inner-key, ...], value)\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AMLPipelineBase.Utils.score-Tuple{Symbol, Vector, Vector}","page":"Types and Functions","title":"AMLPipelineBase.Utils.score","text":"score(metric::Symbol, actual::Vector, predicted::Vector)\n\nScore learner predictions against ground truth values.\n\nAvailable metrics:\n\n:accuracy\nmetric: metric to assess with\nactual: ground truth values\npredicted: predicted values\n\nReturns: score of learner\n\n\n\n\n\n","category":"method"},{"location":"tutorial/extending/#Extending-AutoMLPipeline","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"","category":"section"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"Having a meta-ML package sounds ideal  but not practical  in terms of maintainability and flexibility.  The metapackage becomes a central point of failure and bottleneck. It doesn't subscribe to the KISS philosophy of Unix which encourages decentralization of implementation. As long as the input and output behavior of transformers and learners follow a standard format, they should work without   dependency or communication. By using a consistent input/output interfaces, the passing of information among the elements in the pipeline will not bring any surprises to the receivers and transmitters of information down the line.","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"Because AMPL's symbolic pipeline is based on the idea of Linux pipeline and filters, there is a deliberate effort to follow as much as possible the KISS philosophy by just using two interfaces to be overloaded (fit! and transform!):  input features should be a DataFrame type while the target output should be a Vector type. Transformers fit! function expects only one input argument and ignores the target  argument. On the other hand, the fit! function of any learner  requires both input and target arguments to carry out the  supervised learning phase. For the transform! function, both learners and transformers expect one input argument that both use to apply their learned parameters in transforming the input into either prediction, decomposition, normalization, scaling, etc.","category":"page"},{"location":"tutorial/extending/#AMLP-Abstract-Types","page":"Extending AutoMLPipeline","title":"AMLP Abstract Types","text":"","category":"section"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"The AMLP abstract types are composed of the following:","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"abstract type Machine end\nabstract type Workflow    <:  Machine  end \nabstract type Computer    <:  Machine  end \nabstract type Learner     <:  Computer end\nabstract type Transformer <:  Computer end","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"At the top of the hierarchy is the Machine abstraction that supports two major interfaces: fit! and transform!. The abstract Machine has two major types: Computer and Workflow.  The Computer types perform computations suchs as filters, transformers, and filters while the Workflow controls the flow of information. A Workflow can be a sequential flow of information or a combination of information from two or more workflow. A Workflow that provides sequential flow is called Pipeline (or linear pipeline) while the one that combines information from different workflows is called ComboPipeline.","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"The Computer type has two subtypes: Learner and Transformer. Their main difference is in the behavior of their fit! function. The Learner type learns its parameters by finding a mapping function between its  input and output arguments while the Transformer does not require these mapping function to perform its operation.  The Transfomer learns all its parameters by just processing its input features. Both Transfomer and Learner has similar behaviour in the transform! function. Both apply their learned parameters to transform their input into output.","category":"page"},{"location":"tutorial/extending/#Extending-AMLP-by-Adding-a-CSVReader-Transformer","page":"Extending AutoMLPipeline","title":"Extending AMLP by Adding a CSVReader Transformer","text":"","category":"section"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"Let's extend AMLP by adding CSV reading support embedded in the pipeline. Instead of passing the data in the pipeline argument, we create a csv transformer that passes the data to succeeding elements in the pipeline from a csv file.","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"module FileReaders\n\nusing CSV \nusing DataFrames: DataFrame, nrow,ncol\n\nusing AutoMLPipeline\nusing AutoMLPipeline.AbsTypes # abstract types (Learners and Transformers)\n\nimport AutoMLPipeline.fit!\nimport AutoMLPipeline.transform!\n\nexport fit!, transform!\nexport CSVReader\n\n# define a user-defined structure for type dispatch\nmutable struct CSVReader <: Transformer\n   name::String\n   model::Dict\n\n   function CSVReader(args = Dict(:fname=>\"\"))\n      fname = args[:fname]\n      fname != \"\" || throw(ArgumentError(\"missing filename.\"))\n      isfile(fname) || throw(ArgumentError(\"file does not exist.\"))\n      new(fname,args)\n   end\nend\n\nCSVReader(fname::String) = CSVReader(Dict(:fname=>fname))\n\n# Define fit! which does error checking. You can also make \n# it do nothing and let the transform! function does the\n# the checking and loading. The fit! function is only defined\n# here to make sure there is a fit! dispatch for CSVReader\n# type which is needed in the pipeline call iteration.\nfunction fit!(csvreader::CSVReader, df::DataFrame=DataFrame(), target::Vector=Vector())\n   fname = csvreader.name\n   isfile(fname) || throw(ArgumentError(\"file does not exist.\"))\nend\n\n# define transform which opens the file and returns a dataframe\nfunction transform!(csvreader::CSVReader, df::DataFrame=DataFrame())\n   fname = csvreader.name\n   df = CSV.File(fname) |> DataFrame\n   df != DataFrame() || throw(ArgumentError(\"empty dataframe.\"))\n   return df\nend\nend\nnothing #hide","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"Let's now load the FileReaders module together with the other AutoMLPipeline modules and create a pipeline that includes the csv reader we just created.","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"using DataFrames: DataFrame, nrow,ncol\n\n\nusing AutoMLPipeline\n\nusing .FileReaders # load from the Main module\n\n#### Column selector\ncatf = CatFeatureSelector() \nnumf = NumFeatureSelector()\npca = SKPreprocessor(\"PCA\")\nohe = OneHotEncoder()\n\nfname = joinpath(dirname(pathof(AutoMLPipeline)),\"../data/profb.csv\")\ncsvrdr = CSVReader(fname)\n\np1 = @pipeline csvrdr |> (catf + numf)\ndf1 = fit_transform!(p1) # empty argument because input coming from csvreader\nnothing #hide","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"first(df1,5)","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"p2 = @pipeline csvrdr |> (numf |> pca) + (catf |> ohe)  \ndf2 = fit_transform!(p2) # empty argument because input coming from csvreader\nnothing #hide","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"first(df2,5)","category":"page"},{"location":"tutorial/extending/","page":"Extending AutoMLPipeline","title":"Extending AutoMLPipeline","text":"With the CSVReader extension, csv files can now be directly processed or loaded inside the pipeline and can be used with other existing filters and transformers.","category":"page"},{"location":"tutorial/preprocessing/#Preprocessing","page":"Preprocessing","title":"Preprocessing","text":"","category":"section"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Let us start by loading the diabetes dataset:","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"using AutoMLPipeline\nusing CSV\nusing DataFrames\n\ndiabetesdf = CSV.File(joinpath(dirname(pathof(AutoMLPipeline)),\"../data/diabetes.csv\")) |> DataFrame\nX = diabetesdf[:,1:end-1]\nY = diabetesdf[:,end] |> Vector\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"We can check the data by showing the first 5 rows:","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"show5(df)=first(df,5); # show first 5 rows\nshow5(diabetesdf)","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"This UCI dataset  is a collection of diagnostic tests among the Pima Indians  to investigate whether the patient shows  sign of diabetes or not based on certain features:","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Number of times pregnant\nPlasma glucose concentration a 2 hours in an oral glucose tolerance test\nDiastolic blood pressure (mm Hg)\nTriceps skin fold thickness (mm)\n2-Hour serum insulin (mu U/ml)\nBody mass index (weight in kg/(height in m)^2)\nDiabetes pedigree function\nAge (years)\nClass variable (0 or 1) indicating diabetic or not","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"What is interesting with this dataset is that one or more numeric columns can be categorical and should be hot-bit encoded. One way to verify is  to compute the number of unique instances for each column and look for  columns with relatively smaller count:","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"[n=>length(unique(x)) for (n,x) in pairs(eachcol(diabetesdf))] |> collect","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Among the input columns, preg has only 17 unique instances and it can be treated as a categorical variable. However, its description indicates that the feature refers to the number of times the patient is pregnant and can be considered numerical. With this dilemma, we need to figure out which representation provides better performance to our classifier. In order to test the two options, we can use the Feature Discriminator module to filter and transform the preg column to either numeric or categorical and choose the pipeline with the optimal performance.","category":"page"},{"location":"tutorial/preprocessing/#CatNumDiscriminator-for-Detecting-Categorical-Numeric-Features","page":"Preprocessing","title":"CatNumDiscriminator for Detecting Categorical Numeric Features","text":"","category":"section"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Transform numeric columns with small unique instances to categories.","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Let us use CatNumDiscriminator which expects one argument to indicate the maximum number of unique instances in order to consider a particular column as categorical. For the sake of this discussion, let us use its  default value which is 24.","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"using AutoMLPipeline\n\ndisc = CatNumDiscriminator(24)\n@pipeline disc\ntr_disc = fit_transform!(disc,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"show5(tr_disc)","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"You may notice that the preg column is converted by the CatNumDiscriminator into String type which can be fed to hot-bit encoder to preprocess  categorical data:","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"disc = CatNumDiscriminator(24)\ncatf = CatFeatureSelector()\nohe = OneHotEncoder()\npohe = @pipeline disc |> catf |> ohe\ntr_pohe = fit_transform!(pohe,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"show5(tr_pohe)","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"We have now converted all categorical data into hot-bit encoded values.","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"For a typical scenario, one can consider columns with around 3-10  unique numeric instances to be categorical.  Using CatNumDiscriminator, it is trivial to convert columns of features with small unique instances into categorical and hot-bit encode them as shown below. Let us use 5 as the cut-off and any columns with less than 5 unique instances is converted to hot-bits.","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"using DataFrames: DataFrame, nrow,ncol\n\ndf = rand(1:3,100,3) |> DataFrame;\nshow5(df)\ndisc = CatNumDiscriminator(5);\npohe = @pipeline disc |> catf |> ohe;\ntr_pohe = fit_transform!(pohe,df);\nshow5(tr_pohe)","category":"page"},{"location":"tutorial/preprocessing/#Concatenating-Hot-Bits-with-PCA-of-Numeric-Columns","page":"Preprocessing","title":"Concatenating Hot-Bits with PCA of Numeric Columns","text":"","category":"section"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Going back to the original diabetes dataset, we can now use the  CatNumDiscriminator to differentiate between categorical  columns and numerical columns and preprocess them based on their  types (String vs Number). Below is the pipeline to convert preg column to hot-bits and use PCA for the numerical features:","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"pca = SKPreprocessor(\"PCA\")\ndisc = CatNumDiscriminator(24)\nohe = OneHotEncoder()\ncatf = CatFeatureSelector()\nnumf = NumFeatureSelector()\npl = @pipeline disc |> ((numf |> pca) + (catf |> ohe))\nres_pl = fit_transform!(pl,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"show5(res_pl)","category":"page"},{"location":"tutorial/preprocessing/#Performance-Evaluation","page":"Preprocessing","title":"Performance Evaluation","text":"","category":"section"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Let us compare the RF cross-validation result between two options:","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"preg column should be categorical vs\npreg column is numerical","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"in predicting diabetes where numerical values are scaled by robust scaler and decomposed by PCA.","category":"page"},{"location":"tutorial/preprocessing/#Option-1:-Assume-All-Numeric-Columns-as-not-Categorical-and-Evaluate","page":"Preprocessing","title":"Option 1: Assume All Numeric Columns as not Categorical and Evaluate","text":"","category":"section"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"pca = SKPreprocessor(\"PCA\")\ndt = SKLearner(\"DecisionTreeClassifier\")\nrf = SKLearner(\"RandomForestClassifier\")\nrbs = SKPreprocessor(\"RobustScaler\")\njrf = RandomForest()\nlsvc = SKLearner(\"LinearSVC\")\nohe = OneHotEncoder()\ncatf = CatFeatureSelector()\nnumf = NumFeatureSelector()\ndisc = CatNumDiscriminator(0) # disable turning numeric to categorical features\npl = @pipeline disc |> ((numf |>  pca) + (catf |> ohe)) |> jrf\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"crossvalidate(pl,X,Y,\"accuracy_score\",30)","category":"page"},{"location":"tutorial/preprocessing/#Option-2:-Assume-as-Categorical-Numeric-Columns-24-and-Evaluate","page":"Preprocessing","title":"Option 2: Assume as Categorical Numeric Columns <= 24 and Evaluate","text":"","category":"section"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"disc = CatNumDiscriminator(24) # turning numeric to categorical if unique instances <= 24\npl = @pipeline disc |> ((numf |>  pca) + (catf |> ohe)) |> jrf\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"crossvalidate(pl,X,Y,\"accuracy_score\",30)","category":"page"},{"location":"tutorial/preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"From this evaluation, preg column should be treated as numerical because the corresponding pipeline got better performance. One thing to note is the presence of errors in the cross-validation performance for the pipeline that treats preg as categorical data. The subset of training data during the kfold validation may contain singularities and evaluation causes some errors due to hot-bit encoding that increases data sparsity. The error, however, may be a bug which needs to be addressed in  the future.","category":"page"},{"location":"tutorial/pipeline/#PipelineUsage","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"A tutorial for using the @pipeline expression","category":"page"},{"location":"tutorial/pipeline/#Dataset","page":"Pipeline","title":"Dataset","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let us start the tutorial by loading the dataset.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"using AutoMLPipeline\nusing CSV\nusing DataFrames\n\nprofbdata = getprofb()\nX = profbdata[:,2:end] \nY = profbdata[:,1] |> Vector\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"We can check the data by showing the first 5 rows:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"show5(df)=first(df,5); # show first 5 rows\nshow5(profbdata)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"This dataset is a collection of pro football scores with the following variables and their descriptions:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Home/Away = Favored team is at home or away\nFavorite Points = Points scored by the favored team\nUnderdog Points = Points scored by the underdog team\nPointspread = Oddsmaker's points to handicap the favored team\nFavorite Name = Code for favored team's name\nUnderdog name = Code for underdog's name\nYear = 89, 90, or 91","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"note: Note\nFor the purpose of this tutorial, we will use the first column, Home vs Away, as the target variable to be predicted using the other columns as input features. For this target output, we are trying to ask whether the model can learn the patterns from its input features to predict whether the game was played at home or away. Since the input features have both categorical and numerical features, the dataset is a good basis to describe  how to extract these two types of features, preprocessed them, and learn the mapping using a one-liner pipeline expression.","category":"page"},{"location":"tutorial/pipeline/#AutoMLPipeline-Modules-and-Instances","page":"Pipeline","title":"AutoMLPipeline Modules and Instances","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Before continuing further with the tutorial, let us load the  necessary AutoMLPipeline package:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"using AutoMLPipeline\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let us also create some instances of filters, transformers, and models that we can use to preprocess and model the dataset.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"#### Decomposition\npca = SKPreprocessor(\"PCA\"); fa = SKPreprocessor(\"FactorAnalysis\"); \nica = SKPreprocessor(\"FastICA\")\n\n#### Scaler \nrb = SKPreprocessor(\"RobustScaler\"); pt = SKPreprocessor(\"PowerTransformer\") \nnorm = SKPreprocessor(\"Normalizer\"); mx = SKPreprocessor(\"MinMaxScaler\")\n\n#### categorical preprocessing\nohe = OneHotEncoder()\n\n#### Column selector\ndisc = CatNumDiscriminator()\ncatf = CatFeatureSelector(); numf = NumFeatureSelector()\n\n#### Learners\nrf = SKLearner(\"RandomForestClassifier\"); gb = SKLearner(\"GradientBoostingClassifier\")\nlsvc = SKLearner(\"LinearSVC\"); svc = SKLearner(\"SVC\")\nmlp = SKLearner(\"MLPClassifier\"); ada = SKLearner(\"AdaBoostClassifier\")\njrf = RandomForest(); vote = VoteEnsemble(); stack = StackEnsemble()           \nbest = BestLearner()\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#Processing-Categorical-Features","page":"Pipeline","title":"Processing Categorical Features","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"For the first illustration, let us extract categorical features of  the data and output some of them using the pipeline expression  and its interface:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"pop_cat = @pipeline catf \ntr_cat = fit_transform!(pop_cat,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"show5(tr_cat)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"One may notice that instead of using fit! and transform,  the example uses fit_transform! instead. The latter is equivalent to calling fit! and transform in sequence which is handy for examining the final output of the transformation prior to  feeding it to the model.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let us now transform the categorical features into one-hot-bit-encoding (ohe) and examine the results:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"pop_ohe = @pipeline catf |> ohe\ntr_ohe = fit_transform!(pop_ohe,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"show5(tr_ohe)","category":"page"},{"location":"tutorial/pipeline/#Processing-Numerical-Features","page":"Pipeline","title":"Processing Numerical Features","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let us have an example of extracting the numerical features of the data using different combinations of filters/transformers:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"pop_rb = @pipeline (numf |> rb)\ntr_rb = fit_transform!(pop_rb,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"show5(tr_rb)","category":"page"},{"location":"tutorial/pipeline/#Concatenating-Extracted-Categorical-and-Numerical-Features","page":"Pipeline","title":"Concatenating Extracted Categorical and Numerical Features","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"For typical modeling workflow, input features are combinations of categorical features transformer to one-bit encoding together with numerical features normalized or scaled or transformed by decomposition. ","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Here is an example of a typical input feature:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"pop_com = @pipeline (numf |> norm) + (catf |> ohe)\ntr_com = fit_transform!(pop_com,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"show5(tr_com)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"The column size from 6 grew to 60 after the hot-bit encoding was applied because of the large number of unique instances for the categorical columns. ","category":"page"},{"location":"tutorial/pipeline/#Performance-Evaluation-of-the-Pipeline","page":"Pipeline","title":"Performance Evaluation of the Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"We can add a model at the end of the pipeline and evaluate the performance of the entire pipeline by cross-validation.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let us use a linear SVC model and evaluate using 5-fold cross-validation.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Random.seed!(12345);\npop_lsvc = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> lsvc;\ntr_lsvc = crossvalidate(pop_lsvc,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"What about using Gradient Boosting model?","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Random.seed!(12345);\npop_gb = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> gb;\ntr_gb = crossvalidate(pop_gb,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"What about using Random Forest model?","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Random.seed!(12345);\npop_rf = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> jrf;\ntr_rf = crossvalidate(pop_rf,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let's evaluate several learners which is a typical workflow in searching for the optimal model.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"using Random\nusing DataFrames: DataFrame, nrow,ncol\n\nusing AutoMLPipeline\n\nRandom.seed!(1)\njrf = RandomForest()\nada = SKLearner(\"AdaBoostClassifier\")\nsgd = SKLearner(\"SGDClassifier\")\ntree = PrunedTree()\nstd = SKPreprocessor(\"StandardScaler\")\ndisc = CatNumDiscriminator()\nlsvc = SKLearner(\"LinearSVC\")\n\nlearners = DataFrame()\nfor learner in [jrf,ada,sgd,tree,lsvc]\n  pcmc = @pipeline disc |> ((catf |> ohe) + (numf |> std)) |> learner\n  println(learner.name)\n  mean,sd,_ = crossvalidate(pcmc,X,Y,\"accuracy_score\",10)\n  global learners = vcat(learners,DataFrame(name=learner.name,mean=mean,sd=sd))\nend;\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"@show learners;","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"note: Note\nIt can be inferred from the results that linear SVC has the best performance with respect to the different pipelines evaluated. The compact expression supported by the  pipeline makes testing of the different combination of features  and models trivial. It makes performance evaluation   of the pipeline easily manageable in a systematic way.","category":"page"},{"location":"tutorial/pipeline/#Learners-as-Filters","page":"Pipeline","title":"Learners as Filters","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"It is also possible to use learners in the middle of  expression to serve as filters and their outputs become  input to the final learner as illustrated below.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Random.seed!(1);\nexpr = @pipeline ( \n                   ((numf |> pca) |> gb) + ((numf |> pca) |> jrf) \n                 ) |> ohe |> ada;\n                 \ncrossvalidate(expr,X,Y,\"accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"It is important to take note that ohe is necessary because the outputs of the two learners (gb and jrf)  are categorical values that need to be hot-bit encoded before  feeding them to the final ada learner.","category":"page"},{"location":"tutorial/pipeline/#Advanced-Expressions-using-Selector-Pipeline","page":"Pipeline","title":"Advanced Expressions using Selector Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"You can use * operation as a selector  function which outputs the result of the best learner. Instead of looping over the different learners to identify the best learner, you can use the selector function  to automatically determine the best learner and output its  prediction. ","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Random.seed!(1);\npcmc = @pipeline disc |> ((catf |> ohe) + (numf |> std)) |> \n                 (jrf * ada * sgd * tree * lsvc);\ncrossvalidate(pcmc,X,Y,\"accuracy_score\",10)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Here is another example using the Selector Pipeline as a preprocessor in the feature extraction stage of the pipeline:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Random.seed!(1);\npjrf = @pipeline disc |> ((catf |> ohe) + (numf |> std)) |>\n                 ((jrf * ada ) + (sgd * tree * lsvc)) |> ohe |> ada;\n\ncrossvalidate(pjrf,X,Y,\"accuracy_score\")","category":"page"},{"location":"#AutoMLPipeline-(AMLP)","page":"HOME","title":"AutoMLPipeline (AMLP)","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"is a package that makes it trivial to create  complex ML pipeline structures using simple  expressions. AMLP leverages on the built-in macro programming features of Julia to symbolically process, manipulate  pipeline expressions, and automatically discover optimal structures  for machine learning prediction and classification.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"To illustrate, a typical machine learning workflow that extracts numerical features (numf) for ICA (independent component analysis) and  PCA (principal component analysis) transformations, respectively, concatenated with the hot-bit encoding (ohe) of categorical  features (catf) of a given data for RF modeling can be expressed  in AMLP as:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> model = @pipeline (catf |> ohe) + (numf |> pca) + (numf |> ica) |> rf\njulia> fit!(model,Xtrain,Ytrain)\njulia> prediction = transform!(model,Xtest)\njulia> score(:accuracy,prediction,Ytest)\njulia> crossvalidate(model,X,Y,\"accuracy_score\")\njulia> crossvalidate(model,X,Y,\"balanced_accuracy_score\")","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"You can visualize the pipeline by using AbstractTrees Julia package.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"# package installation\njulia> using Pkg\njulia> Pkg.add(\"AbstractTrees\")\njulia> Pkg.add(\"AutoMLPipeline\")\n\n# load the packages\njulia> using AbstractTrees\njulia> using AutoMLPipeline\n\njulia> expr = @pipelinex (catf |> ohe) + (numf |> pca) + (numf |> ica) |> rf\n:(Pipeline(ComboPipeline(Pipeline(catf, ohe), Pipeline(numf, pca), Pipeline(numf, ica)), rf))\n\njulia> print_tree(stdout, expr)\n:(Pipeline(ComboPipeline(Pipeline(catf, ohe), Pipeline(numf, pca), Pipeline(numf, ica)), rf))\n├─ :Pipeline\n├─ :(ComboPipeline(Pipeline(catf, ohe), Pipeline(numf, pca), Pipeline(numf, ica)))\n│  ├─ :ComboPipeline\n│  ├─ :(Pipeline(catf, ohe))\n│  │  ├─ :Pipeline\n│  │  ├─ :catf\n│  │  └─ :ohe\n│  ├─ :(Pipeline(numf, pca))\n│  │  ├─ :Pipeline\n│  │  ├─ :numf\n│  │  └─ :pca\n│  └─ :(Pipeline(numf, ica))\n│     ├─ :Pipeline\n│     ├─ :numf\n│     └─ :ica\n└─ :rf","category":"page"},{"location":"#Motivations","page":"HOME","title":"Motivations","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"The typical workflow in machine learning  classification or prediction requires  some or combination of the following  preprocessing steps together with modeling:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"feature extraction (e.g. ica, pca, svd)\nfeature transformation (e.g. normalization, scaling, ohe)\nfeature selection (anova, correlation)\nmodeling (rf, adaboost, xgboost, lm, svm, mlp)","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"Each step has several choices of functions to use together with their corresponding  parameters. Optimizing the performance of the entire pipeline is a combinatorial search of the proper order and combination of preprocessing steps, optimization of their corresponding parameters, together with searching for  the optimal model and its hyper-parameters.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"Because of close dependencies among various steps, we can consider the entire process  to be a pipeline optimization problem (POP). POP requires simultaneous optimization of pipeline structure and parameter adaptation of its elements. As a consequence, having an elegant way to express pipeline structure helps in the analysis and implementation of the optimization routines.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"The target of future work will be the  implementations of different pipeline  optimization algorithms ranging from  evolutionary approaches, integer programming (discrete choices of POP elements),  tree/graph search, and hyper-parameter search.","category":"page"},{"location":"#Package-Features","page":"HOME","title":"Package Features","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Pipeline API that allows high-level description of processing workflow\nCommon API wrappers for ML libs including Scikitlearn, DecisionTree, etc\nSymbolic pipeline parsing for easy expression  of complex pipeline structures\nEasily extensible architecture by overloading just two main interfaces: fit! and transform!\nMeta-ensembles that allows composition of    ensembles of ensembles (recursively if needed)    for robust prediction routines\nCategorical and numerical feature selectors for    specialized preprocessing routines based on types","category":"page"},{"location":"#Installation","page":"HOME","title":"Installation","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"AutoMLPipeline is in the Julia Official package registry.  The latest release can be installed at the Julia  prompt using Julia's package management which is triggered by pressing ] at the julia prompt:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> ]\n(v1.0) pkg> add AutoMLPipeline","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> pkg\"add AutoMLPipeline\"","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> Pkg.add(\"AutoMLPipeline\")","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"Once AutoMLPipeline is installed, you can  load it by:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> using AutoMLPipeline","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> import AutoMLPipeline","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"Generally, you will need the different learners/transformers and utils in AMLP for to carry-out the processing and modeling routines. ","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"using AutoMLPipeline \nusing AutoMLPipeline.FeatureSelectors\nusing AutoMLPipeline.EnsembleMethods\nusing AutoMLPipeline.CrossValidators \nusing AutoMLPipeline.DecisionTreeLearners\nusing AutoMLPipeline.Pipelines\nusing AutoMLPipeline.BaseFilters\nusing AutoMLPipeline.SKPreprocessors \nusing AutoMLPipeline.Utils`","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"CSV and DataFrames will be needed in the succeeding examples and should be installed:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"using Pkg\nPkg.add(\"CSV\")\nPkg.add(\"DataFrames\")","category":"page"},{"location":"#Tutorial-Outline","page":"HOME","title":"Tutorial Outline","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Pages = [\n  \"tutorial/pipeline.md\",\n  \"tutorial/preprocessing.md\",\n  \"tutorial/learning.md\",\n  \"tutorial/extending.md\"\n]\nDepth = 3","category":"page"},{"location":"#Manual-Outline","page":"HOME","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Pages = [\n  \"man/pipeline.md\",\n  \"man/preprocessors.md\",\n  \"man/learners.md\",\n  \"man/metaensembles.md\"\n]\nDepth = 3","category":"page"},{"location":"#ML-Library","page":"HOME","title":"ML Library","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Pages = [\n  \"lib/typesfunctions.md\"\n]","category":"page"},{"location":"man/learners/#Learners","page":"Learners","title":"Learners","text":"","category":"section"},{"location":"man/learners/","page":"Learners","title":"Learners","text":"Similar to SKPreprocessor, most of the Learners in AMLP for its initial release are based on Scikitlearn libraries.","category":"page"},{"location":"man/learners/","page":"Learners","title":"Learners","text":"note: Note\nFor more information and specific details of arguments to pass and learner's behaviour, please consult the Scikitlearn  documentation.","category":"page"},{"location":"man/learners/#SKLearner-Structure","page":"Learners","title":"SKLearner Structure","text":"","category":"section"},{"location":"man/learners/","page":"Learners","title":"Learners","text":"    SKLearner(Dict(\n       :name => \"sklearner\",\n       :output => :class,\n       :learner => \"LinearSVC\",\n       :impl_args => Dict()\n      )\n    )\n\nHelper Function:\n    SKLearner(learner::String,args::Dict=Dict())","category":"page"},{"location":"man/learners/","page":"Learners","title":"Learners","text":"SKLearner maintains a dictionary of learners which can be listed by invoking the function: sklearners() The :impl_args is a dictionary of paramters to be passed as arguments to the Scikitlearn learner.","category":"page"},{"location":"man/learners/","page":"Learners","title":"Learners","text":"Let's try loading some learners with some arguments based on Scikitlearn documentation:","category":"page"},{"location":"man/learners/","page":"Learners","title":"Learners","text":"using AutoMLPipeline\n\niris = getiris();\nX = iris[:,1:4];\nY = iris[:,end] |> Vector;\n\nrf = SKLearner(\"RandomForestClassifier\",Dict(:n_estimators=>30,:random_state=>0));\ncrossvalidate(rf,X,Y,\"accuracy_score\",3)\n\nada = SKLearner(\"AdaBoostClassifier\",Dict(:n_estimators=>20,:random_state=>0));\ncrossvalidate(ada,X,Y,\"accuracy_score\",3)\n\nsvc = SKLearner(\"SVC\",Dict(:kernel=>\"rbf\",:random_state=>0,:gamma=>\"auto\"));\ncrossvalidate(svc,X,Y,\"accuracy_score\",3)","category":"page"},{"location":"man/metaensembles/#Ensemble-Methods","page":"Meta-Ensembles","title":"Ensemble Methods","text":"","category":"section"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"AMPL supports three types of meta-ensembles, namely:  StackEnsemble, VoteEnsemble, and BestLearner. They are considered as meta-ensembles because they can contain other learners including other ensembles as well as meta-ensembles. They support complex level of hierarchy depending on the requirements. The most effective way to show their flexibility is to provide some real examples.","category":"page"},{"location":"man/metaensembles/#StackEnsemble","page":"Meta-Ensembles","title":"StackEnsemble","text":"","category":"section"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"Stack ensemble uses the idea of stacking to train  learners into two stages. The first stage trains bottom-level learners for the mapping  between the input and output. The default is to use 70% of the data. Once the bottom-level learners finish the training,  the algorithm proceeds to stage 2 which treats the trained learners as transformers. The output from  these transformers is used to train the Meta-Learner (RandomForest, PrunedTree, or Adaboost) using the remaining 30% of the data. ","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"The StackEnsemble accepts the following arguments wrapped in a Dictionary type argument:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":":name -> alias name of ensemble\n:learners -> a vector of learners\n:stacker -> the meta-learner (RandomForest, or Adaboost, or PrunedTree)\n:stacker_training_portion -> percentage of data for the meta-learner\n:keep_original_features -> boolean (whether the original data is included together with the transformed data by the bottom-level learners)","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"While the init function of StackEnsemble expects an argument of Dictionary type, it supports the following convenient function signatures:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"StackEnsemble(Dict(:learners=>...,:stacker=>...))\nStackEnsemble([learner1,learner2,...],Dict(:stacker=>...))\nStackEnsemble([learner1,learner2,...],stacker=...)\nStackEnsemble([learner1,learner2,...])","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"To illustrate, let's create some bottom-level learners from Scikitlearn and Julia:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"using AutoMLPipeline\nusing DataFrames\n\ngauss = SKLearner(\"GaussianProcessClassifier\")\nsvc = SKLearner(\"LinearSVC\")\nridge = SKLearner(\"RidgeClassifier\")\njrf = RandomForest() # julia's rf\nrfstacker = RandomForest()\nstackens = StackEnsemble([gauss,svc,ridge,jrf],stacker=rfstacker)\nnothing #hide","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"Let's load some dataset and create a pipeline with the stackens as the learner at the end of the pipeline.","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"using CSV\nusing Random\nRandom.seed!(123);\n\nprofbdata = CSV.File(joinpath(dirname(pathof(AutoMLPipeline)),\"../data/profb.csv\")) |> DataFrame\nX = profbdata[:,2:end] \nY = profbdata[:,1] |> Vector;\n\nohe = OneHotEncoder()\ncatf = CatFeatureSelector();\nnumf = NumFeatureSelector()\nrb = SKPreprocessor(\"RobustScaler\"); \npt = SKPreprocessor(\"PowerTransformer\");\npca = SKPreprocessor(\"PCA\"); \nfa = SKPreprocessor(\"FactorAnalysis\"); \nica = SKPreprocessor(\"FastICA\")\npplstacks = @pipeline  (numf |> rb |> pca) + (numf |> rb |> ica) + \n                       (catf |> ohe) + (numf |> rb |> fa) |> stackens\nnothing #hide","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"crossvalidate(pplstacks,X,Y)","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"It is worth noting that stack ensemble is dealing with mixture of libraries consisting of Julia's Random Forest and Scikitlearn learners.","category":"page"},{"location":"man/metaensembles/#VoteEnsemble","page":"Meta-Ensembles","title":"VoteEnsemble","text":"","category":"section"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"Vote ensemble uses similar idea with the Stack Ensemble  but instead of stacking, it uses voting to get the final prediction. The first stage involves the collection of  bottom-level learners being trained to learn the mapping between input and output. Once they are trained in a classification problem, they are treated as transformers  wherein the final output of the ensemble is based on the  output with the greatest count. It's equivalent to majority  voting where each learner has one vote based on its prediction output class.","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"The VoteEnsemble accepts the following arguments wrapped inside a Dictionary type of argument:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":":name -> alias name of ensemble\n:learners -> a vector of learners","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"While the init function of VoteEnsemble expects a Dictionary type of argument, it also supports the following convenient helper functions:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"VoteEnsemble(Dict(:learners=>...,:name=>...))\nVoteEnsemble([learner1,learner2,...],Dict(:name=>...))\nVoteEnsemble([learner1,learner2,...],name=...)\nVoteEnsemble([learner1,learner2,...])","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"Let's use the same pipeline but substitute the stack ensemble with the vote ensemble:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"Random.seed!(123);\n\nvotingens = VoteEnsemble([gauss,svc,ridge,jrf]);\npplvote = @pipeline  (numf |> rb |> pca) + (numf |> rb |> ica) + \n                     (catf |> ohe) + (numf |> rb |> fa) |> votingens;\nnothing #hide","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"crossvalidate(pplvote,X,Y)","category":"page"},{"location":"man/metaensembles/#bestlearner","page":"Meta-Ensembles","title":"BestLearner","text":"","category":"section"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"The BestLearner ensemble does not perform any 2-stage mapping. What it does is to cross-validate each learner performance and use the most optimal learner as the final model. This ensemble can be used to automatically pick the  most optimal learner in a group of learners included in each argument based on certain selection criteria.","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"The BestLearner accepts the following arguments wrapped in Dictionary type argument:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":":selection_function ->  Function\n:score_type         -> Real\n:partition_generator -> Function\n:learners            -> Vector of learners\n:name                -> alias name of learner\n:learner_options_grid -> for hyper-parameter search","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"The BestLearner supports the following function signatures aside from Dictionary type argument:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"BestLearner(Dict(:learners=>...,:name=>...))\nBestLearner([learner1,learner2,...],Dict(:name=>...))\nBestLearner([learner1,learner2,...],name=...)\nBestLearner([learner1,learner2,...])","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"Let's use the same pipeline as above but substitute the vote ensemble with the BestLearner ensemble:","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"Random.seed!(123);\n\nbestens = BestLearner([gauss,svc,ridge,jrf]);\npplbest = @pipeline  (numf |> rb |> pca) + (numf |> rb |> ica) + \n                     (catf |> ohe) + (numf |> rb |> fa) |> bestens;\nnothing #hide","category":"page"},{"location":"man/metaensembles/","page":"Meta-Ensembles","title":"Meta-Ensembles","text":"crossvalidate(pplbest,X,Y)","category":"page"}]
}
